<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title type="text" xml:lang="en">site_title</title>
  <link type="application/atom+xml" href="http://arrdem.com/atom.xml" rel="self"/>
  <link type="text/html" href="http://arrdem.com" rel="alternate"/>
  <updated>2015-02-05T04:41:02+00:00</updated>
  <id>Barely typed</id>
  <author>
    <name>Reid McKenzie</name>
  </author>
  <rights>&copy; 2015. All rights reserved.</rights>
  
  <entry>
    <title>Toothpick: A theory of bytecode machines</title>
    <link href="http://arrdem.com/2015/01/28/toothpick:_a_theory_of_bytecode_machines/"/>
    <updated>2015-01-28T22:50:37+00:00</updated>
    <id>http://arrdem.com/2015/01/28/toothpick:_a_theory_of_bytecode_machines</id>
    <content type="html">&lt;p&gt;In working on my &lt;a href=&quot;/2014/01/10/Batbridge/&quot;&gt;Batbridge&lt;/a&gt;
simulators and other adventures in computing hardware, one of the
problems I have repeatedly encountered is the need to generate bit
encodings of symbolic instructions. When working with &amp;quot;well known&amp;quot;
architectures with existing tooling support, one can usually find an
&amp;quot;off the shelf&amp;quot; assembler either in the GCC collection or
elsewhere. However when you&amp;#39;re working against a custom instruction
set you&amp;#39;re on your own for tooling.&lt;/p&gt;

&lt;p&gt;So lets invent yet another assembler!&lt;/p&gt;

&lt;p&gt;An assembler is simply a program that describes how to construct a
binary instruction encoding from a set of symbolic human-writable
mnemonics representing data manipulating directives to computing
hardware. &lt;code&gt;add&lt;/code&gt;, &lt;code&gt;sub[tract]&lt;/code&gt;, &lt;code&gt;mul[tiply]&lt;/code&gt; and &lt;code&gt;jump&lt;/code&gt; are good
examples of such mnemonics. Each operation which a given &amp;quot;machine&amp;quot;
(hardware/software interpreter) will accept is typically specified as
a sequence of bits and bit encoded fields (sometimes abbreviated in
octal or hexadecimal notation). So Batbridge for instance specifies
that the add instruction is the bit string
&lt;code&gt;110000tttttaaaaabbbbbiiiiiiiiiii&lt;/code&gt; where the prefix &lt;code&gt;110000&lt;/code&gt; or &lt;code&gt;0x30&lt;/code&gt;
indicates the addition operation, the bits named &lt;code&gt;t&lt;/code&gt; encode a number
&lt;code&gt;t∈[0..30]&lt;/code&gt; being the register to which the result of the addition
will be stored, the bits named &lt;code&gt;a&lt;/code&gt; subject to the same constraints as
&lt;code&gt;t&lt;/code&gt; name the register from which the &amp;quot;left&amp;quot; operand will be drawn and
the &lt;code&gt;b&lt;/code&gt; bits name the &amp;quot;right&amp;quot; operand same as the &lt;code&gt;t&lt;/code&gt; and &lt;code&gt;a&lt;/code&gt;
operands. &lt;code&gt;i&lt;/code&gt; is an arbitrary signed 10 bit integer (sign is the 11th
bit).&lt;/p&gt;

&lt;p&gt;Sitting down with
&lt;a href=&quot;https://github.com/arrdem/batbridge/blob/master/doc/batbridge.md&quot;&gt;a specification&lt;/a&gt;,
you or I could construct a set of functions from values appropriately
constrained to bit encoded instructions as laid out in an instruction
set specification. However in doing such an assembler implementation,
you will discover that you are simply encoding in the programming
language of your choice a translation table laid out in full by the
instruction set documentation. However realizing that the assembler
which we are constructing by hand represents a printer to the bytecode
machine&amp;#39;s reader, we can think of bytecodes as a language in the same
sense that any other set of strings and productions constitutes a
language.&lt;/p&gt;

&lt;h2&gt;A Theory of Bytecode Machines&lt;/h2&gt;

&lt;p&gt;It happens that we have good formal theories about languages and their
structures. If we stop and squint, we can see that the individual
opcodes are analogous to terminals or verbs. If we allow ourselves to
model opcodes as words in a language, then a program (a production of
words) is a sentence for which we can establish a grammar. For
instance a ELF formatted program could be considered to be a sequence
of blocks of instructions (verbs) defined in terms of words (opcodes)
and formatted with an appropriate header/footer (the ELF file
structure). As these are all linguistic constructs, a program capable
of generating these values must be a direct production of the
specification which lays out these productions in the same way that a
lex/yacc lexer parser pair is a mechanical production of the lexing
and parsing rules which define a computer language.&lt;/p&gt;

&lt;p&gt;This leads to the idea that, just as we now rarely write parsers and
lexers preferring to derive them automatically from specifications of
the languages we wish them to process since a as suggested above a
bytecode is simply a language on bit strings rather than ASCII or
Unicode characters the consequent that we can use the same class of
tools to generate assemblers and disassemblers as we use to generate
parsers and printers should be obvious. We just need to construct
formal languages describing the instructions our machines accept.&lt;/p&gt;

&lt;h2&gt;An Assembler Generator&lt;/h2&gt;

&lt;p&gt;Just as we have theories about &amp;quot;interesting&amp;quot; sets of languages and the
parser techniques which may be applied to efficiently recognize and
process them, so too rather than being able to automatically assemble
arbitrary programs to arbitrary bytecodes we must first recognize that
as a bytecode is a language on bit strings and as there exist
languages which require a Turing machine to recognize in addition to
uncomputable languages consequently one could invent an &amp;quot;interesting&amp;quot;
(but I will argue in a minute silly) bytecode for which no assembler
or recognizer can be automatically generated.&lt;/p&gt;

&lt;p&gt;So what features define &amp;quot;interesting&amp;quot; bytecodes? In hardware terms
&lt;a href=&quot;/2014/01/01/Hardware-101/&quot;&gt;as I discuss elsewhere&lt;/a&gt;
memories and lookup tables are just about the most expensive thing you
can build in terms of chip area and time. As a result, the
overwhelming majority of computer architectures are not what I shall
call &amp;quot;micro-programmable&amp;quot;, that is users cannot give meaning to new
sequences of bits and rather interact with the computer by chaining
together a few bit-verbs or opcodes which are built into the
chip. There do exist such micro-programmable computers, FPGAs, but
they tend to be special purpose prototyping machines and are not in
wide usage compared to fixed instruction set machines.&lt;/p&gt;

&lt;p&gt;Another characteristic of &amp;quot;interesting&amp;quot; bytecodes is in the name,
bytes or words. Many years have gone by since bit addressed machines
were last built with commercial intent. Instead commercial machines
and off the shelf memories tend to address &amp;quot;blocks&amp;quot; of bits or &amp;quot;lines&amp;quot;
being strings a power of two in length which may be divided into
substrings smaller powers of two in length as the user may see fit at
no additional hardware complexity cost by simply selecting bits
through selecting predefined subdivisions of lines known as &amp;quot;words&amp;quot; or
using bit masking and shifting to select strings smaller than a single
word. Strings larger than a single word must be dealt with by using
multiple operations on words.&lt;/p&gt;

&lt;p&gt;This theory of a fixed word size, fixed instruction set machine
characterizes the overwhelming majority of instruction sets and
machines designed and built by Intel, ARM, Symbolics, Thinking
Machines, and others over the past decades due mainly to the ease of
implementation and efficiency of such designs.&lt;/p&gt;

&lt;p&gt;So what does this theory about a &amp;quot;useful&amp;quot; bytecode machine buy us? It
implies that we can describe a &amp;quot;useful&amp;quot; machine as a bitstring length
denoting word size. Endianness must also be considered. Then given an
enumeration of the various opcodes and how to generate them as
bitstrings, you can completely describe an &amp;quot;interesting&amp;quot; instruction
set as characterized above.&lt;/p&gt;

&lt;p&gt;Assemblers are simply programs that deal with translating instructions
set mnemonics writable by a programmer or more often than not a
compiler to bytecodes which a machine can execute. The details of
computing label addresses are entirely for the most part independent
of the specific architecture in so much as they are determined by
properties of the specific target bytecode described above. This
suggests that there must exist a function&lt;/p&gt;

&lt;p&gt;&lt;code&gt;(assemble p ← Platform c ← Codes)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Which makes sense if you consider a single target assembler to be the
partial application of such an assembler to a platform.&lt;/p&gt;

&lt;h2&gt;Talk is cheap, show me the code&lt;/h2&gt;

&lt;p&gt;First things first, we need a way to represent an &amp;quot;interesting&amp;quot;
instruction set.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-clojure&quot; data-lang=&quot;clojure&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;deftype &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;Architecture&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&amp;quot;A structure representing an \&amp;quot;interesting\&amp;quot; bytecode architecture&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;    from which a parser or a generator can be computed.&amp;quot;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;word-width&lt;/span&gt;         &lt;span class=&quot;c1&quot;&gt;;; ← Num, how many bits in a word&lt;/span&gt;
   &lt;span class=&quot;nv&quot;&gt;pointer-resolution&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;; ← Num, how many bits forwards does *(x+1) go&lt;/span&gt;
   &lt;span class=&quot;nv&quot;&gt;opcodes&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;;; ← Map [Memonic → Opcode], opcode formatting rules&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So what do individual opcodes look like? We&amp;#39;re gonna need some bit
vector formatting engine to do real code bytecode generation with.&lt;/p&gt;

&lt;p&gt;I will define an opcode to be a bitstring &lt;em&gt;of fixed length&lt;/em&gt; (this is
important, there are instruction sets with variable length &amp;quot;opcodes&amp;quot;
however as the different length forms have different operational
semantics I shall define them to be different operations and declare
the instruction set architects silly people) composed of a sequence of
concatenated parameters or fields and constants which may be either
signed or unsigned. Fields are named or anonymous. The set of named
field names in a single opcode must have no repetitions. We want to be
able to write something like the following&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-clojure&quot; data-lang=&quot;clojure&quot;&gt;&lt;span class=&quot;c1&quot;&gt;;; IFLT 0x20  100000 _____ aaaaa bbbbb iiiiiiiiiii&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;;; execute the next instruction IFF (&amp;lt; a b)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;opcode&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:iflt&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;const-field&lt;/span&gt;            &lt;span class=&quot;ss&quot;&gt;:icode&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;x20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;enforced-const-field&lt;/span&gt;   &lt;span class=&quot;ss&quot;&gt;:_&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;parameter-field&lt;/span&gt;        &lt;span class=&quot;ss&quot;&gt;:a&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;  &lt;span class=&quot;nv&quot;&gt;register?&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;parameter-field&lt;/span&gt;        &lt;span class=&quot;ss&quot;&gt;:b&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;  &lt;span class=&quot;nv&quot;&gt;register?&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;signed-parameter-field&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:i&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;literal?&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So in this example instruction from Batbridge, we define an
instruction as the ordered bit string where the bits &lt;code&gt;[0..5]&lt;/code&gt; are
named &lt;code&gt;:icode&lt;/code&gt; and fixed with the constant value of &lt;code&gt;0x20&lt;/code&gt;. We then
have an anonymous field 5 bits long taking the bits &lt;code&gt;[6..10]&lt;/code&gt; which we
force to have the value of &lt;code&gt;0&lt;/code&gt; since they are defined to be ignored by
the instruction set. We then have a pair of (unsigned) 5-bit
registers, which must satisfy some guard predicate ensuring that the
value in question fits within the value domain which the instruction
set allows for registers. Finally we have a signed field 11 bits long,
which again is guarded with a value domain predicate. This notation
encodes the bit format string, as well as the value domains for each
element of the bit format string in a form that can be easily
inspected by either an assembler or a disassembler program.&lt;/p&gt;

&lt;p&gt;So what does that expression build out to as a data structure?&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-clojure&quot; data-lang=&quot;clojure&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:width&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;,
 &lt;span class=&quot;ss&quot;&gt;:params&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:_&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:a&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:b&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;,
 &lt;span class=&quot;ss&quot;&gt;:fields&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:offset&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;26&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:width&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:name&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:icode&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:type&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:const&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:value&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;35&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:offset&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:width&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:name&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:_&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:type&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:enforced-const&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:value&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:offset&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:width&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:name&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:a&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:type&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:unsigned-field&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;&amp;lt;batbridge$register_QMARK_&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;toothpick.isa.batbridge$register_QMARK_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;bd4095&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:offset&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:width&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:name&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:b&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:type&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:unsigned-field&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;&amp;lt;batbridge$register_QMARK_&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;toothpick.isa.batbridge$register_QMARK_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;bd4095&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:offset&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:width&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:name&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:i&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:type&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:signed-field&lt;/span&gt;,
           &lt;span class=&quot;ss&quot;&gt;:pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;&amp;lt;batbridge$literal_QMARK_&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;toothpick.isa.batbridge$literal_QMARK_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;38334886&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Given this datastructure, and a sequence &lt;code&gt;(opcode . params)&lt;/code&gt;, we can
trivially compute a map &lt;code&gt;(zipmap (:params (get isa opcode)) params)&lt;/code&gt;,
and then fold right over the fields of the instruction computing each
field then shifting and anding it into place in the bit vector to
construct the bit encoding of the instruction.&lt;/p&gt;

&lt;p&gt;As implemented this is a little fragile because it assumes that the
computer running the assembler assembler has a larger or equal integer
size than the target, as attempting to construct an int aka bit vector
that&amp;#39;s too large will yield... interesting issues. Future fixes are to
use a real sequence of bits, or to rewrite this logic in terms of
multiplication by two and addition.&lt;/p&gt;

&lt;p&gt;Deriving a parser for this structure is also (fairly)
straightforwards, one simply can simply generate an alternation
matcher on bitstring matchers matching the individual
instructions. &amp;quot;interesting&amp;quot; ISAs tend to put the opcode specification
in the &amp;quot;low&amp;quot; bits of each instruction word so as a we can simply
generate a jump table by masking on the least significant bits but
that&amp;#39;s not implemented yet.&lt;/p&gt;

&lt;p&gt;Also this representation falls into &lt;a href=&quot;http://www.jneen.net/&quot;&gt;jneen&lt;/a&gt;&amp;#39;s
&lt;a href=&quot;https://www.youtube.com/watch?v=ZQkIWWTygio&quot;&gt;type tagging pitfall&lt;/a&gt;,
but this is old code and implementation details thereof so I&amp;#39;ll
forgive myself and fix it later.&lt;/p&gt;

&lt;p&gt;We can then define an &amp;quot;architecture&amp;quot; by constructing a composite map
as above having a word size and pointer interval specification and a
map from keywords naming instructions to many such instruction maps.&lt;/p&gt;

&lt;p&gt;So, we can describe an instruction stream as a sequence of
instructions, and we can assemble individual instructions by
interpreting from their representation above, so if we wanted to
encode a sample program&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-clojure&quot; data-lang=&quot;clojure&quot;&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:label&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:top&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:op&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:add&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:param&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:register&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:param&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:const&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:param&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:const&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]]]&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:label&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:op&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:add&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:param&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:register&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:param&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:const&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:param&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:const&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]]]&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:label&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:op&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:add&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:param&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:register&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:param&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:register&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:param&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:register&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]]]&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:label&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can simply foldr a label location computing operation since all of
our opcodes are defined to be fixed length even if they are abstractly
variable length, then in a second pass we assemble each instruction
against the label location context and the instruction set, giving us
a sequence of bytecodes. Done! ∎&lt;/p&gt;

&lt;p&gt;This is essentially my
&lt;a href=&quot;https://github.com/arrdem/toothpick&quot;&gt;Toothpick&lt;/a&gt; project, which seeks
to provide exactly this meta-assembler facility and has already proven
its worth to me in automating the generation of assemblers for the
various toy architectures with which I&amp;#39;ve worked at school. It&amp;#39;s not
done yet and as noted above it needs some spit and shoeshine but I
think it represents a remarkably simple and powerful idea.&lt;/p&gt;

&lt;p&gt;The real power of this approach is that, extended, this can enable the
automatic generation of LLVM backends! If an instruction specification
were to include its operational semantics written in terms of LLVM
instructions one could imagine a trivial derived LLVM backed which
sequentially scans emitted LLVM assembly matching out the platform
translation of the instruction &amp;quot;at point&amp;quot; and then assembling the
resulting instruction stream as above. Since LLVM already provides a
massively portable C compiler infrastructure, this means that given
only a formal platform specification one could construct a mediocre C
compiler only from the platform specification.&lt;/p&gt;

&lt;p&gt;To take this idea even further, one could also imagine generating a
platform simulator from either symbolic instructions or bit encoded
instructions from only the augmented ISA with semantics instructions.&lt;/p&gt;

&lt;p&gt;Generating muxing logic in Verilog or VHDL should also be trivial from
this structure.&lt;/p&gt;

&lt;p&gt;Generate all the things! Correctness by specification &amp;amp; construction!
Toy architectures for everyone!&lt;/p&gt;

&lt;p&gt;^d&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Ox: Some motivation</title>
    <link href="http://arrdem.com/2015/01/27/ox:_some_motivation/"/>
    <updated>2015-01-27T05:41:54+00:00</updated>
    <id>http://arrdem.com/2015/01/27/ox:_some_motivation</id>
    <content type="html">&lt;p&gt;Here is some of my motivation for Oxlang. I&amp;#39;m sorry it isn&amp;#39;t shorter, I gave up
on copy editing to turn in. This was originally a personal email and appears
almost unaltered.&lt;/p&gt;

&lt;p&gt;Clojure is awesome. It is grammatically simple with a classical lispy syntax,
and its cheap immutable data structures enable equational reasoning about most
problems and solution tools.&lt;/p&gt;

&lt;p&gt;My top bugbears &amp;amp; goals:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Contributor culture. The contribution process should be as open, friendly and
low friction as possible
&lt;a href=&quot;https://www.youtube.com/watch?x-yt-cl=84637285&amp;amp;v=_ahvzDzKdB0&amp;amp;feature=player_detailpage&amp;amp;x-yt-ts=1422040409#t=1033&quot;&gt;see Guy Steele&amp;#39;s comments&lt;/a&gt;,
on growing a better language from one with warts through acceptance of
contributions. The GHC team has a &amp;quot;3% rule&amp;quot;, in that if a change improves the
language or the type checker and comes at a cost of less than a 3%
&lt;strong&gt;slowdown&lt;/strong&gt; it has a high probability of being accepted and
acceptance/rejection is publicly debated ala LKML. Commentary on the Clojure
process may be inferred.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Formalism. Formalism gives portabity and is the only real rout
thereto. Parser spec. Evaluator spec. Kernel standard library spec with a
features list. This enables portability of code and data structures. Keeps us
out of the JVM centric mess that Clojure itself is close to right now.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Documentation. I built &lt;a href=&quot;htttp://conj.io&quot;&gt;Grimoire&lt;/a&gt; and will say no more on
this subject. Goes with formalism requirements.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hide the host. That formalism should include the call stack and
exceptions. Use the host under the hood sure, but don&amp;#39;t leak the host for the
most part. To paraphrase Paul Phillips, Just because Java or Clojure jumped
off a bridge doesn&amp;#39;t mean we can&amp;#39;t have nice things like &lt;code&gt;Data.Ord&lt;/code&gt;. This
likely means making interop deliberately second class. Possible even
explicitly generated from introspection but not the common case. Musings on
this may be found &lt;a href=&quot;/2014/08/26/on_future_languages/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Types (and protocols/interfaces) matter. Clojure and much of the Cognitect
crew seem to pretend that they don&amp;#39;t. It feels like not a week goes by
without someone in IRC or twitter posting a function that checks the run time
type of some value against some implementation type of Clojure&amp;#39;s
core. &lt;a href=&quot;https://twitter.com/aphyr/status/559874479099097088&quot;&gt;Aphyr obligingly posted today&amp;#39;s&lt;/a&gt;,
&lt;a href=&quot;https://github.com/clojure/core.incubator/blob/master/src/main/clojure/clojure/core/incubator.clj#L83-L92&quot;&gt;&lt;code&gt;clojure.core.incubator/sequable&lt;/code&gt;&lt;/a&gt;,
&lt;a href=&quot;https://github.com/arrdem/detritus/blob/master/src/detritus/types.clj&quot;&gt;I have a few&lt;/a&gt;. Enable
static reasoning rather than pretending it doesn&amp;#39;t matter and that nobody
does it. Bake in type inference and something akin to prismatic/schema.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Purify the language implementation. Practice what you preach. Oxlang/Kiss&amp;#39;s
immutable environments absolutely required. Death to &lt;code&gt;&amp;amp;env&lt;/code&gt;, &lt;code&gt;*ns*&lt;/code&gt; and
mutable namespaces, do reloading by dependency tracking.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Give thought to being self-hosting. It&amp;#39;s not essential off the bat, but it
makes porting &amp;amp; compatability easier in the long run. GHC&amp;#39;s bootstrap &amp;amp;
rebuild itself process beccons and Toccata plays the same trick.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Give more thought to the module system &amp;amp; versioning culture. I want to be
able to depend on &amp;quot;something implementing these functions in the namespace
&lt;code&gt;clojure.core&lt;/code&gt; then require those and alias them in as such&amp;quot;, not &amp;quot;Dear lein,
this exact version of this library, thanks no ranges can&amp;#39;t trust anyone. Okay
lets just load crap up&amp;quot;. The language should &lt;em&gt;enforce&lt;/em&gt; versioning b/c
programmers sure as hell can&amp;#39;t be trusted with it. Note that this goes with
my first point to lower the cost(s) of churn in the standard library.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tooling. I&amp;#39;m with Chas in his comments
&lt;a href=&quot;https://twitter.com/cemerick/status/558287166397509632&quot;&gt;here&lt;/a&gt;. As hinted in
7, the language needs to provide &lt;em&gt;as much support&lt;/em&gt; as possible for editors,
for REPLs, for packages, for optimizing, for testing. These are the things
that make or break the UX of the language and if the UX isn&amp;#39;t good people
will just go back to writing node.js or whatever the latest flavor of the
week is. Tooling is hard and takes time, but it must be a goal.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All of these (save the parser and evaluator spec) are goals towards which work
can be accumulated over time once the semantics of the language are agreed
upon. Tooling, types, speed, correctness analysis can all be done if changes
(growth) is cheap and the goals are agreed upon.&lt;/p&gt;

&lt;p&gt;I think that given these changes it would be pretty easy to design embedded and
memory restricted targets for &amp;quot;Clojure&amp;quot;, and I&amp;#39;ll admit that the goal of writing
an OS with this language
&lt;a href=&quot;/2014/11/28/the_future_of_the_lispm/&quot;&gt;is near to my heart&lt;/a&gt; but for
the most part I want to improve the platform independent experience since most
of the code I write isn&amp;#39;t platform bound it&amp;#39;s exploratory scratch work.&lt;/p&gt;

&lt;p&gt;^d&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Oxcart going forwards</title>
    <link href="http://arrdem.com/2014/12/11/oxcart_going_forwards/"/>
    <updated>2014-12-11T14:35:07+00:00</updated>
    <id>http://arrdem.com/2014/12/11/oxcart_going_forwards</id>
    <content type="html">&lt;p&gt;When I &lt;a href=&quot;/2014/08/06/of_oxen,_carts_and_ordering/&quot;&gt;last wrote&lt;/a&gt; about
&lt;a href=&quot;https://github.com/oxlang/oxcart&quot;&gt;Oxcart&lt;/a&gt; work pretty much went on hiatus due
to my return to school. As there has been some recent interest in the status of
Lean Clojure overall I thought I&amp;#39;d take the opportunity to review the state of
Oxcart and the plan for Oxcart going forwards.&lt;/p&gt;

&lt;h2&gt;Oxcart and Clojure&lt;/h2&gt;

&lt;p&gt;The static linking approach and lack of run time dynamism found in Oxcart is
explicitly at odds with the philosophy of core Clojure. Where Clojure was
designed to enable live development and makes performance sacrifices to enable
such development
&lt;a href=&quot;http://stackoverflow.com/questions/27388758/what-is-the-difference-between-clojure-repl-and-scala-repl/27391533#27391533&quot;&gt;as discussed here&lt;/a&gt;,
Oxcart attempts to offer the complement set of trade offs. Oxcart is intended as
a pre-deployment static compiler designed to take a working application and to
the greatest extent possible wring more performance out of unchanged (but
restricted) Clojure as PyPy does for Python. As Oxcart explicitly avoids the
dynamic bindings which Clojure embraces,
&lt;a href=&quot;http://tech.puredanger.com/&quot;&gt;Alex Miller&lt;/a&gt;, the Clojure Community Manager, has
repeatedly stated that he expects to see little cross pollination from Oxcart
and related work to Clojure itself.&lt;/p&gt;

&lt;p&gt;This would be all well and good, were it not for the existing behavior of
Clojure&amp;#39;s &lt;code&gt;clojure.lang.RT&lt;/code&gt; class. As currently implemented in Clojure 1.6 and
1.7, &lt;code&gt;RT&lt;/code&gt; uses its &lt;code&gt;&amp;lt;initc&amp;gt;&lt;/code&gt; method to compile the following resources with
&lt;code&gt;clojure.lang.Compiler&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&amp;quot;clojure/core&amp;quot;&lt;/li&gt;
&lt;li&gt;&amp;quot;clojure/core_proxy&amp;quot;&lt;/li&gt;
&lt;li&gt;&amp;quot;clojure/core_print&amp;quot;&lt;/li&gt;
&lt;li&gt;&amp;quot;clojure/genclass&amp;quot;&lt;/li&gt;
&lt;li&gt;&amp;quot;clojure/core_deftype&amp;quot;&lt;/li&gt;
&lt;li&gt;&amp;quot;clojure/core/protocols&amp;quot;&lt;/li&gt;
&lt;li&gt;&amp;quot;clojure/gvec&amp;quot;&lt;/li&gt;
&lt;li&gt;&amp;quot;clojure/instant&amp;quot;&lt;/li&gt;
&lt;li&gt;&amp;quot;clojure/uuid&amp;quot;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These represent about 10799 lines of code, all of which could easily be
statically compiled and most importantly tree shaken ahead of time by Oxcart or
another tool rather than being loaded at boot time. This also means that the
unavoidable booting of Clojure itself from source can easily dominate loading
user programs especially after static compilation to raw classes. A quick
benchmark on my machine shows that booting a Clojure 1.6 instance, loading a ns
containing only a &lt;code&gt;-main&lt;/code&gt; that only prints &amp;quot;hello world&amp;quot; takes ~2.8 seconds from
source compared to ~2.5 seconds booting the same program compiled with Oxcart
suggesting that the cost of booting Clojure is the shared ~2.5 second boot
time. This is the &lt;code&gt;test.hello&lt;/code&gt; benchmark in
&lt;a href=&quot;https://github.com/oxlang/oxcart#demos&quot;&gt;Oxcart&amp;#39;s demos&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ git clone git@github.com:oxlang/oxcart.git &amp;amp;&amp;amp;\
  cd oxcart &amp;amp;&amp;amp;\
  git checkout 0.1.2 &amp;amp;&amp;amp;\
  bash bench.sh test.hello
Running Clojure 1.6.0 compiled test.hello....
Hello, World!

real    0m1.369s
user    0m3.117s
sys     0m0.083s
Oxcart compiling test.hello....
Running Oxcart compiled test.hello....
Hello, World!

real    0m1.212s
user    0m2.487s
sys     0m0.073s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then there&amp;#39;s the &lt;code&gt;test.load&lt;/code&gt; benchmark. This benchmark as-is pushes credulity
because it compiles 502 functions of which only the &lt;code&gt;-main&lt;/code&gt; which uses none of
the other 501 will be invoked. This reflects more on program loading time than
on the loading time of &amp;quot;clojure/core&amp;quot;, but I still think instructive in the
costs of boot time compilation, showing a ~7s boot time for Clojure compared to
a ~2.5s boot time for Oxcart. As arbitrary slowdowns from macroexpansions which
&lt;code&gt;Thread/sleep&lt;/code&gt; would be entirely possible I consider this program within the
bounds of &amp;quot;fairness&amp;quot;.&lt;/p&gt;

&lt;h2&gt;A Fork in the Road&lt;/h2&gt;

&lt;p&gt;There are two solutions to this limitation, and both of them involve changing
the behavior of Clojure itself. The first is my proposed
&lt;a href=&quot;https://groups.google.com/forum/#!searchin/clojure-dev/lib-clojure/clojure-dev/dSPUNKSaV94/gTikbqYhJTYJ&quot;&gt;lib-clojure&lt;/a&gt;
refactor. Partitioning Clojure is a bit extreme, and in toying with the proposed
&lt;code&gt;RT&lt;/code&gt; → &lt;code&gt;Util&lt;/code&gt; changes &lt;a href=&quot;https://github.com/arrdem/clojure/tree/arrdem&quot;&gt;over here&lt;/a&gt;
I&amp;#39;ve found that they work quite nicely even with a monolithic Clojure
artifact. Unfortunately there seems to be little interest from Clojure&amp;#39;s Core
team (as judged via Alex&amp;#39;s communications over the last few months) in these
specific changes or in the static compilation approach to reducing the
deployment overhead of Clojure programs. The second is to fork Clojure and then
make lib-clojure changes which solves the problem of convincing Core that
lib-clojure is a good idea but brings its own suite of problems.&lt;/p&gt;

&lt;p&gt;Oxcart was intended to be my undergraduate thesis work. While the 16-25% speedup
previously reported is impressive, Oxcart does nothing novel or even interesting
under the hood. It only performs four real program transformations:
&lt;a href=&quot;http://en.wikipedia.org/wiki/Lambda_lifting&quot;&gt;lambda lifting&lt;/a&gt;, two kinds of
static call site linking and
&lt;a href=&quot;http://en.wikipedia.org/wiki/Unreachable_code&quot;&gt;tree shaking&lt;/a&gt;. While I suppose
impressive for an undergrad, this project also leaves a lot on the table in
terms of potential utility due to its inability to alter &lt;code&gt;RT&lt;/code&gt;&amp;#39;s unfortunate
loading behavior. I also think there is low hanging fruit in doing unreachable
form elimination and effect analysis, probably enough that Oxcart as-is would
not be &amp;quot;complete&amp;quot; even were its emitter more stable.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m reluctant to simply fork Clojure, mainly because I don&amp;#39;t think that the
changes I&amp;#39;ve been kicking about for lib-clojure actually add anything to Clojure
as a language. If I were to fork Clojure, it&amp;#39;d be for
&lt;a href=&quot;/2014/09/10/ox:_a_preface/&quot;&gt;Oxlang&lt;/a&gt;
which actually seeks to make major changes to Clojure not just tweak some
plumbing. But writing a language so I can write a compiler is frankly silly so
that&amp;#39;s not high on the options list. The worst part of this is that forking
Clojure makes everything about using Oxcart harder. Now you have dependencies at
build time (all of &amp;quot;stock&amp;quot; Clojure) that don&amp;#39;t exist at deployment time (my
&amp;quot;hacked&amp;quot; Clojure). Whatever hack that requires either winds up complicating
everyone&amp;#39;s &lt;code&gt;project.clj&lt;/code&gt; or in an otherwise uncalled for leiningen plugin just
like &lt;a href=&quot;https://github.com/alexander-yakushev/lein-skummet&quot;&gt;lein-skummet&lt;/a&gt;. Tooling
needs to be able to get around this too when every library you&amp;#39;d want to use
explicitly requires &lt;code&gt;[org.clojure/clojure ...]&lt;/code&gt; which totally goes away once
Oxcart emits the bits you need and throws the rest out. Most of all I don&amp;#39;t want
to maintain a fork for feature parity as time goes on. However I also don&amp;#39;t see
any other a way to get around &lt;code&gt;RT&lt;/code&gt;&amp;#39;s existing behavior since the &lt;code&gt;RT&lt;/code&gt; → &lt;code&gt;Util&lt;/code&gt;
refactor touches almost every java file in Clojure.&lt;/p&gt;

&lt;h2&gt;Flaws in the Stone&lt;/h2&gt;

&lt;p&gt;Oxcart itself also needs a bunch of work. While I think that Nicola has done an
awesome job with &lt;code&gt;tools.analyzer&lt;/code&gt; and &lt;code&gt;tools.emitter.jvm&lt;/code&gt; I&amp;#39;m presently
convinced that while it&amp;#39;s fine for a naive emitter (what TEJVM is), it&amp;#39;s a
sub-optimal substrate for a whole program representation and for whole program
transforms.&lt;/p&gt;

&lt;p&gt;Consider renaming a local symbol. In the LLVM compiler infrastructure, &amp;quot;locals&amp;quot;
and other program entities are represented as mutable nodes to which references
are held by clients (say call sites or use sites). A rename is then simply an
update in place on the node to be changed. All clients see the change with no
change in state. This makes replacements, renames and so forth constant time
updates. Unfortunately due to the program model used by &lt;code&gt;tools.analyzer&lt;/code&gt; and
&lt;code&gt;tools.emitter.jvm&lt;/code&gt;, such efficient updates are not possible. Instead most
rewrites degenerate into worst case traversals of the entire program AST when
they could be much more limited in
scope. &lt;a href=&quot;https://github.com/oxlang/cutaway&quot;&gt;Cutaway&lt;/a&gt; is one experiment in this
direction, but it at best approximates what &lt;code&gt;clojure.core.logic.pldb&lt;/code&gt; is capable
of. I hope that over Christmas I&amp;#39;ll have time to play with using &lt;code&gt;pldb&lt;/code&gt; to
store, search and rewrite a &amp;quot;flattened&amp;quot; form of &lt;code&gt;tools.analyzer&lt;/code&gt; ASTs.&lt;/p&gt;

&lt;p&gt;Oxcart is out of date with &lt;code&gt;tools.emitter.jvm&lt;/code&gt; and &lt;code&gt;tools.analyzer&lt;/code&gt;. This
shouldn&amp;#39;t be hard to fix, but I just haven&amp;#39;t kept up with Nicola&amp;#39;s ongoing work
over the course of the last semester. This will probably get done over Christmas
as well.&lt;/p&gt;

&lt;p&gt;Oxcart doesn&amp;#39;t support a bunch of stuff. As of right now, &lt;code&gt;defmulti&lt;/code&gt;,
&lt;code&gt;defmethod&lt;/code&gt;, &lt;code&gt;deftype&lt;/code&gt;, &lt;code&gt;defprotocol&lt;/code&gt;, &lt;code&gt;proxy&lt;/code&gt;, &lt;code&gt;extend-type&lt;/code&gt; and
&lt;code&gt;extend-protocol&lt;/code&gt; aren&amp;#39;t supported. I&amp;#39;m pretty sure all of these actually work,
or could easily work, they just didn&amp;#39;t get done in the GSoC time frame.&lt;/p&gt;

&lt;p&gt;Finally and I think this is the thing that&amp;#39;s really blocking me from working on
Oxcart: it can&amp;#39;t compile &lt;code&gt;clojure.core&lt;/code&gt; anyway. This is a huge failing on my
part in terms of emitter completeness, but it&amp;#39;s a moot point because even if I
can compile &lt;code&gt;clojure.core&lt;/code&gt; with Oxcart &lt;code&gt;RT&lt;/code&gt; is gonna load it anyway at boot
time. I also suspect that this is an incompleteness in the project as a whole
which probably makes it an unacceptable thesis submission although I haven&amp;#39;t
spoken with my adviser about it yet.&lt;/p&gt;

&lt;h2&gt;The Endgame&lt;/h2&gt;

&lt;p&gt;As of right now I think it&amp;#39;s fair to call Oxcart abandoned. I don&amp;#39;t think it&amp;#39;s a
worthwhile investment of my time to build and maintain a language fork that
doesn&amp;#39;t have to be a fork. I talked with &lt;a href=&quot;http://www.bytopia.org/&quot;&gt;Alexander&lt;/a&gt;,
one of the clojure-android developers and a fellow GSoC Lean Clojure
student/researcher about this stuff and the agreement we reached was that until
1.7 is released there&amp;#39;s no way that the lib-clojure changes will even get
considered and that the most productive thing we can do as community members is
probably to wait for 1.8 planning and then try to sell lib-clojure and related
cleanup work on the basis of enabling clojure-android and lean
clojure/Oxcart. Practically speaking in terms of my time however, if it&amp;#39;s going
to be a few months until 1.7 and then a year until 1.8, that only gives leaves
me my last semester of college to work on Oxcart against an official version of
Clojure that can really support it. If that&amp;#39;s what it takes to do Oxcart I&amp;#39;ll
likely just find a different thesis project or plan on graduating without a
thesis.&lt;/p&gt;

&lt;pre&gt;
(with-plug
  That said, serious interest in Oxcart as a deployment tool or another
  contributor would probably be enough to push me over the futility hump
  of dealing with a Clojure fork and get Oxcart rolling.)
&lt;/pre&gt;

&lt;p&gt;^d&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>The Future of the LispM</title>
    <link href="http://arrdem.com/2014/11/28/the_future_of_the_lispm/"/>
    <updated>2014-11-28T05:10:38+00:00</updated>
    <id>http://arrdem.com/2014/11/28/the_future_of_the_lispm</id>
    <content type="html">&lt;p&gt;This is also addressed to &lt;a href=&quot;https://twitter.com/lojikil&quot;&gt;@lojikil&lt;/a&gt; towards whom I
have threatened to write this post on multiple occasions. It is expected to piss
off &lt;a href=&quot;http://loper-os.org&quot;&gt;loper-os&lt;/a&gt;. Above all, it represents my opinions backed
up only by a cursory research effort. You have been warned.&lt;/p&gt;

&lt;h2&gt;Background Of Historical LispMs&lt;/h2&gt;

&lt;p&gt;LispMs occupy a transitional period of computing industry history between time
shared mainframes and the single user workstations that would become today&amp;#39;s
desktop computers. Prior to John McCarthy&amp;#39;s invention of time sharing, all
computers had been single program machines which multiple users shared via batch
scheduling. The introduction of time sharing opened the way to exploring how to
make computers useful interactively for many users sitting at terminals. Under
bach processing, users would present fully formed programs written by hand or
with little tool assistance to computer techs who would run the programs and
return printed output.&lt;/p&gt;

&lt;p&gt;The concept of time sharing and interactivity lead to the development of more
dynamic programming environments including the Read Eval Print Loop. However at
the same time, the falling costs of integrated circuits (and DoD funding for
research into the design of such VLSI machines) began to put the first
workstations, single user machines featuring large memories, within the
financial reach of well funded AI laboratories which previously dependent on
time shared mainframes.&lt;/p&gt;

&lt;p&gt;In order to improve interactive performance, Richard Greenblatt and Thomas
Knight developed the &lt;code&gt;cons&lt;/code&gt; machine, the first of the MIT Lisp machines. Priced
at ~$50,000 in 1970s dollars
&lt;a href=&quot;ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-444.pdf&quot;&gt;cons&lt;/a&gt; and the
successor &lt;a href=&quot;ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-528.pdf&quot;&gt;cadr&lt;/a&gt;
machine were single user workstations built primarily it seems to address the
restrictions on memory and memory performance and subsequent detrimental impact
on general program performance faced on time shared systems due to the large
memory footprint of Lisp programs and page thrashing. Hardware/microcode support
was added for maintaining the relatively complex Lisp environment structures,
and for many of the primitive lookup/load/store operations on cons cell
structures that characterize a traditional Lisp implementation&amp;#39;s data access
pattern. This combination of machine support and microcoding enabled Lisp
&amp;quot;compilers&amp;quot; to be far simpler and offloaded the responsibility for maintaining
complicated structures such as the environment from the compiler to the
microcode system. Best of all as the microcode closely corresponded relatively
directly to Lisp, on the rare occasions that truly writing microcode for say
device drivers was called for doing so was easy because users could
transparently invoke and define microcode from their native Lisp environment.&lt;/p&gt;

&lt;p&gt;Seeing a business opportunity, in the early &amp;#39;80s a number of MIT AI lab
researchers departed and founded Symbolics Inc. and Lisp Machines Inc. for the
purpose of building and selling Knight machine derived Lisp
workstations. Ultimately however, Lisp machines did fall out of favor during the
AI winter(s) and lost to what we now call the personal computer. I have seen the
argument made that they were too big-ticket and missed the mass market as a
result. Thus our story really begins.&lt;/p&gt;

&lt;h2&gt;Of Modern Hardware&lt;/h2&gt;

&lt;p&gt;In the years since the demise of the dedicated LispMs, Intel and the other major
chip manufacturers have progressed from single cycle to pipelined, cache
accelerated, superscalar and out of order machines. Knowledge of these designs
is assumed. I suggest my
&lt;a href=&quot;/2014/01/10/Batbridge/&quot;&gt;earlier introduction series&lt;/a&gt; to these
topics if you are not already familiar with them. At a high level however, each
step of this progression trades off chip area, transistors (complexity) and
power budget to wring more instruction level parallelism out of existing
programs.&lt;/p&gt;

&lt;p&gt;The Symbolics 3600 technical report from &amp;#39;83 claims a best case cycle time of
180ns (~5MHz) and a worst case of 250ns (4MHz). Arbitrary reads from main memory
are priced at 600ns (3x the cost of a single instruction). This low (by modern
standards) slowdown between the processor and memory meant that one could write
programs that did lots and lots of random memory access and still have
reasonable performance expectations. However, as a stack machine architecture,
there is no opportunity for the instruction level parallelism exploited by
modern architectures as the exact state of the stack which is depended on by
every instruction changes with every instruction. This forces all program
execution to wait during slow operations like main memory reads, division and
floating point math due to the sequential data dependency on the stack. This is
a fundamental architectural failure common to all stack machines and skirted by
modern software stack machines like the JVM by compiling literal stack
instructions to a register machine in order to recover instruction level
parallelism.&lt;/p&gt;

&lt;p&gt;Modern processors claim instruction times of 0.3̅ns for a 3GHz machine, often
running multiple instructions per cycle. Unfortunately while processors core
clocks have gotten faster since the days of the 3600, memories have not
fundamentally improved. On modern hardware, 100ns for a random main memory read
seems to be more or less normal. This represents a 27x speedup in processor
speed mismatched with a 6x speedup in main memory latency for an effective 4.5x
slowdown on memory reads. Now this is admittedly worst case memory
latency. Thanks to the L1 and L2 cache layers, memory read times of 1ns to 3ns
are not uncommon meaning that for cache efficient programs it is quite
reasonable to expect less than a 2ns or 6 cycles for memory reads hitting in the
L1 or L2 cache.&lt;/p&gt;

&lt;p&gt;I mention this, because techniques like
&lt;a href=&quot;http://cpsc.yale.edu/sites/default/files/files/tr362.pdf&quot;&gt;cdr coding&lt;/a&gt; serve to
bring the average case of list access and construction down from their worst
case behavior of generating massive heap fragmentation (and thus defeating
caching and prefetching) towards the behavior of nicely caching sequential data
layouts (classical arrays) typically via
&lt;a href=&quot;http://www.hypirion.com/musings/understanding-persistent-vector-pt-1&quot;&gt;tree like structures&lt;/a&gt;
or &lt;a href=&quot;http://srfi.schemers.org/srfi-101/srfi-101.html&quot;&gt;SRFI-101&lt;/a&gt;. While
traditional linked list structures due to potential heap fragmentation provide
potentially atrocious cache locality and this cripple the performance of modern
cache accelerated machines, more modern datastructures can simultaneously
provide the traditional cons list pattern while improving cache locality and
thus performance characteristics. &lt;a href=&quot;http://vimeo.com/6624203&quot;&gt;Guy Steele himself&lt;/a&gt;
has even come out arguing this point.&lt;/p&gt;

&lt;p&gt;Without literal implementation as a stack machine traversing linked lists and
providing special hardware support for binding and soforth, the LispM instantly
looses much of the vaunted simplicity which makes it attractive to program
directly and becomes simply another register machine in the mold of modern ARM
or Intel machines with strange long running microcoded instructions reminiscent
more of the VAXen than of modern RISC inspired processors. In short due to the
many performance limitations of linked lists as data structures we as an
industry will never again build machines as the original LispMs were for the
sole purpose of traversing and manipulating linked list data structures. Modern
hardware simply offers better performance with more general applicability.&lt;/p&gt;

&lt;h2&gt;Of Modern Languages&lt;/h2&gt;

&lt;p&gt;We&amp;#39;ve also come a long way in language design and implementation. Compilers,
once slow, have gotten faster and smarter. Virtual machines like the JVM,
JavaScript and the CLR are becoming widely used deployment targets. The ML and
Haskell families of languages have introduced us to concepts of real abstract
types and abstract effects which can be used to build programs coupled only by
the abstract properties of the data being consumed, generated and effects
produced. Type inference is even making such fancy behavior manageable by mere
mortals, while providing language implementations with more and more information
with which to perform both program level optimization and micro-optimization not
possible in traditional naive lisps.&lt;/p&gt;

&lt;h2&gt;Of LispMs Future&lt;/h2&gt;

&lt;p&gt;While we&amp;#39;ll never build a hardware LispM again, I suspect that we will see LispM
like systems return one day in the not too distant future. Not as hardware, but
as software or a virtual machine designed to run atop existing and entirely
adequate modern hardware. Now in 10 years someone may make a commercial hardware
LispM at which point I will be happy to eat my words, but as of right now I
don&amp;#39;t see it happening.&lt;/p&gt;

&lt;p&gt;The JVM and the .net CLR have proven to be amazing platforms. While their
allocation requirements perhaps prohibit their use for implementing operating
systems and driver code
(&lt;a href=&quot;http://research.microsoft.com/en-us/projects/singularity/&quot;&gt;not&lt;/a&gt;
&lt;a href=&quot;http://cosmos.codeplex.com/&quot;&gt;that&lt;/a&gt;
&lt;a href=&quot;http://www.jnode.org/&quot;&gt;people&lt;/a&gt;
&lt;a href=&quot;https://github.com/jtauber/cleese/&quot;&gt;haven&amp;#39;t&lt;/a&gt;
&lt;a href=&quot;http://www.barrelfish.org/&quot;&gt;tried&lt;/a&gt;
this sort of thing)
they do offer excellent and standardized platforms. It is my personal belief
that, by leveraging the flexibility that Lisp dialects have for both data driven
DSLs and macro DSLs that it would be more or less trivial to implement an
operating system using a DSL for generating platform specific assembly. As the
&amp;quot;kernel&amp;quot; OS is implemented in the final &amp;quot;host&amp;quot; LispM language editing the kernel
is possible albeit difficult due to the inherent difficulties of hot swapping
operating systems.&lt;/p&gt;

&lt;p&gt;Add a JIT (no this isn&amp;#39;t easy), preferably implemented in the same assembler DSL
as the host operating system, and the stage is set for building a LispM
environment as a virtual machine running atop modern hardware and using modern
JIT to achieve reasonable performance characteristics. From this vantage the way
is clear to implementing the rest of the operating system and user land in a
fully fledged lisp with good reloading and introspection support atop this
kernel of a JIT and enough of an OS to provide memory and process management.&lt;/p&gt;

&lt;p&gt;This is, I think, an entirely reasonable and practical project for a smallish
(~3 or fewer) team and a single target architecture (Intel). There was a
project,
&lt;a href=&quot;https://web.archive.org/web/20120702085030/http://www.stripedgazelle.org/joey/dreamos.html&quot;&gt;Dream&lt;/a&gt;,
an &lt;a href=&quot;http://people.csail.mit.edu/jaffer/r4rs_toc.html&quot;&gt;r4rs&lt;/a&gt; x86 assembler dsl in
which an r4rs interpreter and operating system were implemented. It booted and
worked more or less fine, and while it lacked polish I think it serves as an
excellent proof of concept that such a self hosting Lisp &lt;code&gt;(assembler os
runtime)&lt;/code&gt; triple is viable. I don&amp;#39;t argue that such an implementation will be
trivially portable to a wide variety of target hardware because as experience
shows porting nominally portable Linux, FreeBSD or FreeRTOS is actually quite
hard and porting a JIT can at best be as hard due to tight coupling with the
specific behavior of the target machine but this is why we specify an abstract
machine and then let implementations deal with the mess as the JVM does.&lt;/p&gt;

&lt;p&gt;I also think that Lisp as a language family could stand to learn some new tricks
if someone were to make the effort to build a full Lispy OS/VM
thing.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://clojure.org&quot;&gt;Clojure&lt;/a&gt; is awesome. I&amp;#39;ve written a boatload of it. It&amp;#39;s an
amazingly simple and elegant tool, and its persistent/immutable datastructures
are absolutely worth stealing as is its concept of hygenic macros via symbol
qualification and probably the concept of protocols/interfaces which Clojure
kinda inherits from its primary host Java.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://http://racket-lang.org/&quot;&gt;Racket&lt;/a&gt; is also awesome although I can&amp;#39;t claim
to have written a bunch of it. Its approach to static compilation, strong
support for documentation and examples via
&lt;a href=&quot;http://docs.racket-lang.org/scribble-pp/index.html&quot;&gt;Scribble&lt;/a&gt;, its pattern
matching and typechecking facilities are totally worth stealing.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.shenlanguage.org/&quot;&gt;Shen&lt;/a&gt; is interesting if only due to its
built in Prolog engine enabling search for arbitrary proofs with
regards to arbitrary program properties. While criticized by the
Haskell community for disregarding completeness being able to write
and search for proofs with regards to arbitrary properties of programs
not just types is I think an amazingly powerful one albeit an active
research area.&lt;/p&gt;

&lt;h2&gt;But is it a LispM?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://www.loper-os.org/?p=42&quot;&gt;There are some people&lt;/a&gt; who will rail against
this vision of mine that the &amp;quot;OS&amp;quot; is as low as we need to try and push a
language. To my mind, the advantages of pushing a language to the hardware level
are scant enough as argued above that it simply does not justify the investment
or the delay. Even the OS may be too low to push a language and that while the
adventure of building an OS with a language to host the language ala
&lt;a href=&quot;http://www.oberon.ethz.ch/&quot;&gt;Oberon&lt;/a&gt; because while integrating the language with
the OS does offer maximal conceptual unity across the combined platform and
provide a huge leap in ease of integrating pipelines of different applications
it also forcibly discards programs not written in &lt;code&gt;HOST_LANG&lt;/code&gt; for
&lt;code&gt;HOST_PLATFORM&lt;/code&gt;. This is a problem if only because the Unix model of computing
as implemented on Linux with the help of the GNU user land is second only to the
mach hiding inside of Mac OS X in terms of apparent developer
adoption. Developers booting the next Dream for the first time won&amp;#39;t find
familiar text editors or tools. It will be a brave new world.&lt;/p&gt;

&lt;p&gt;Maybe that&amp;#39;s a good thing. Maybe we can escape the legacy of Unix with untyped
byte streams and instead revive some of Plan 9 with a basis in lispy goodness
and with more types everywhere. Maybe there&amp;#39;s even a place in this for algebraic
effects. Maybe we can join &lt;a href=&quot;http://doc.urbit.org/&quot;&gt;urbit&lt;/a&gt; in looking towards a
functional, fully netwoked future. Maybe. If only we can look past the obsolete
machines and operating systems of yesteryear with which we seem content to
circlejerk and move forwards with appropriate context and vision.&lt;/p&gt;

&lt;p&gt;Or maybe we&amp;#39;ll just be stuck with Linux, Unix, OpenSSL and the rest of our
somewhat suspect tower of legacy code out of sheer inertia and continue getting
caught up in language of the month rages out of collective ADD.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt; We have not wings, we cannot soar;
       But we have feet to scale and climb
 By slow degrees, by more and more,
       The cloudy summits of our time.

 The mighty pyramids of stone
       That wedge-like cleave the desert airs,
 When nearer seen, and better known,
       Are but gigantic flights of stairs.

 The distant mountains, that uprear
       Their solid bastions to the skies,
 Are crossed by pathways, that appear
       As we to higher levels rise.

 The heights by great men reached and kept
       Were not attained by sudden flight,
 But they, while their companions slept,
       Were toiling upward in the night.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;~ &lt;a href=&quot;http://www.poetryfoundation.org/poem/173902&quot;&gt;Longfellow&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;^d&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Syscall overhead and VMs</title>
    <link href="http://arrdem.com/2014/11/25/syscall_overhead_and_vms/"/>
    <updated>2014-11-25T09:10:58+00:00</updated>
    <id>http://arrdem.com/2014/11/25/syscall_overhead_and_vms</id>
    <content type="html">&lt;p&gt;Recently, a friend of mine at ARM has been kicking around the idea
that syscalls are inefficient and that a more than reasonable speedup
could be achieved across the board in *nix operating systems by
providing a mechanism for reducing the overhead of syscalls.&lt;/p&gt;

&lt;p&gt;This friend of mine is a bit of a joker, but the story he tells to
motivate his idea is a good one.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Say I have a yard full of logs, and I want them chopped. So I walk
out, find a guy with an axe, tell him that I have somew work for him
and we come to an agreement on the price. So he goes out back, chops
a log, I pay him, and just as he&amp;#39;s about to step off the curb and
walk away I grab him, and mention that I have another log for him to
chop on the same terms. When he finally gets home to his wife, who
asks him how his day went, the log chopper goes &amp;quot;I worked for this
crazy guy who had a yard full of wood to chop but told me what to do
one log at a time&amp;quot;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Unfortunately, this does indeed represent the state of syscalls in
most unix based operating systems. There isn&amp;#39;t a command for saying
&amp;quot;stat everything in this directory&amp;quot; for instance, although this is
precisely the implementation of the &lt;code&gt;ls&lt;/code&gt; command. Instead, you &lt;code&gt;stat&lt;/code&gt;
the target directory, yielding an array of files, which you then
sequentially &lt;code&gt;stat&lt;/code&gt; to get individual file properties. This is all
well and good, until you realize that in doing this you&amp;#39;ve incurred
&lt;code&gt;O(N)&lt;/code&gt; system call context switches. It has to be &lt;code&gt;O(N)&lt;/code&gt; on the number
of files, because that&amp;#39;s what &lt;code&gt;ls&lt;/code&gt; does, but the system call context
switches are pure overhead.&lt;/p&gt;

&lt;p&gt;My response to this idea was simply that what you really want to do is
treat the system calls of the OS as its own virtual machine. A system
call as currently envisioned is simply a single op against this
standardized interface, with the system interrupt/context switch
overhead implicit in executing that single op. Consequently, what we
really want to do is not make a context switch into the OS with a
single op in mind, instead we want a bytecode VM, Turing complete or
otherwise, that represents the syscall interface as an interpreter or
a JIT. Making a syscall consequently becomes an exercise in generating
a bytecode program representing all the work you can do in one go in
the context of system call privilage. So take the &lt;code&gt;ls&lt;/code&gt; example. Rather
than making &lt;code&gt;N+1&lt;/code&gt; stat calls, instead our &amp;quot;fast ls&amp;quot; could simply
generate a bytecode program that would do all &lt;code&gt;N+1&lt;/code&gt; syscall ops in a
single go by representing the function&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-clojure&quot; data-lang=&quot;clojure&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;fn &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:-&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;Dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;-&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;stat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This gets really fun when you start thinking about what this means in
the context of an operating system where the bytecode machine exposed
by the OS for syscalls is the same as the bytecode machine exposed for
general programming. Suddenly, you get the OS as almost an exokernel
that you can call out to with lambda functions just as if it were
nonprivilaged code. Given a JIT this really gets fun because as a
program must start running at the highest privilage level it will ever
require, it must be safe to hoist the entire program to that
level. Consequently a JIT could actually inline out the syscall
privilage check, performing it once at program start and then simply
running the entire program with OS privilages in OS context. Clearly
this works better for a managed language VM than for a hardware
language VM with pointer crafting &lt;code&gt;:P&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Interestingly, it turns out that this is not a new idea. Apparently
some old IBM research machines actually had a full interpreting stack
machine baked into the OS kernel to do exactly this, but I haven&amp;#39;t
been able to track down public information on such an operating
system.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>牛: the environment model</title>
    <link href="http://arrdem.com/2014/10/27/ox:_the_environment_model/"/>
    <updated>2014-10-27T08:17:00+00:00</updated>
    <id>http://arrdem.com/2014/10/27/ox:_the_environment_model</id>
    <content type="html">&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: Oxlang is vaporware. It may exist some day, there is
some code, however it is just my thought experiment at polishing out
some aspects of Clojure I consider warts by starting from a tabula
rasa. The following represents a mostly baked scheme for implementing
&lt;code&gt;ns&lt;/code&gt; and &lt;code&gt;require&lt;/code&gt; more nicely in static (CLJS, Oxcart) rather than
dynamic (Clojure) contexts.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Unlike Clojure in which the unit of compilation is a single form,
Oxlang&amp;#39;s unit of compilation is that of a &amp;quot;namespace&amp;quot;, or a single
file. Oxlang namespaces are roughly equivalent to Haskell modules in
that they are a comprised of a &amp;quot;header&amp;quot;, followed by a sequence of
declarative body forms.&lt;/p&gt;

&lt;p&gt;In Clojure, the &lt;a href=&quot;http://conj.io/1.6.0/clojure.core/ns/&quot;&gt;&lt;code&gt;ns&lt;/code&gt;&lt;/a&gt; form
serves to imperatively create and initialize a namespace and binding
scope. This is done by constructing a new anonymous function, using it
as a class loader context to perform cached compilation of depended
namespaces. Subsequent forms are compiled as they occur and the
results are accumulated as globally visible &lt;code&gt;def&lt;/code&gt;s.&lt;/p&gt;

&lt;p&gt;Recompiling or reloading a file does exactly that. The &lt;code&gt;ns&lt;/code&gt; form is
re-executed, incurring more side-effects, and all forms in the file
are re-evaluated generating more &lt;code&gt;def&lt;/code&gt;s. However this does not discard
the old &lt;code&gt;def&lt;/code&gt;s from the same file, nor purge the existing aliases and
refers in the reloaded namespace. This can lead to interesting bugs
where changes in imports and &lt;code&gt;def&lt;/code&gt;s create name conflicts with the
previous imports and cause reloading to fail. The failure to
invalidate deleted &lt;code&gt;def&lt;/code&gt;s also creates conditions where for instance
during refactorings the old name for a function remains interred and
accessible the program run time allowing evaluation of code which
depends on the old name to succeed until the entire program is
reloaded in a fresh run time at which point the missing name will
become evident as a dependency fault.&lt;/p&gt;

&lt;p&gt;Furthermore, the &lt;code&gt;Var&lt;/code&gt; mechanism serves to enable extremely cheap code
reloading because all bindings are dynamically resolved anyway. This
means that there is exactly zero recompilation cost to new code beyond
compilation of the new code itself since the &lt;code&gt;Var&lt;/code&gt; look up operation
is performed at invoke time rather than at assemble time.&lt;/p&gt;

&lt;p&gt;Unfortunately in my Clojure development experience, the persistence of
deleted symbols resulted in more broken builds than I care to
admit. Building and maintaining a dependency graph between symbols is
computationally inexpensive, is a key part of many language level
analyses for program optimization and here critically provides better
assurance that REPL development behavior is identical to program
behavior in a cold program boot context.&lt;/p&gt;

&lt;p&gt;In order to combat these issues, two changes must be made. First,
re-evaluating a &lt;code&gt;ns&lt;/code&gt; form must yield a &amp;quot;fresh&amp;quot; environment that cannot
be tainted by previous imports and bindings. This resolves the import
naming conflict issues by making them impossible. By modeling a
&amp;quot;namespace&amp;quot; as a concrete &amp;quot;module&amp;quot; value having dependencies, public
functions and private functions we can mirror the imperative semantics
enabled by Clojure&amp;#39;s &lt;code&gt;def&lt;/code&gt;s and &lt;code&gt;Var&lt;/code&gt;s simply by accumulating
&amp;quot;definitions&amp;quot; into the &amp;quot;module&amp;quot; as they are compiled.&lt;/p&gt;

&lt;p&gt;This model isn&amp;#39;t a total gain however due to the second change, that
reloading entirely (and deliberately) invalidates the previous
definitions of every symbol in the reloaded namespace by swapping out
the old namespace definition for the new one. This implies that other
namespaces/modules which depend on a reloaded module must themselves
be reloaded in topological sort order once the new dependencies are
ready requiring dependency tracking and reloading infrastructure far
beyond Clojure&amp;#39;s (none). Naively this must take place on a file by
file basis as in Scala, however by tracking file change time stamps of
source files and the hash codes of individual def forms a reloading
environment can prove at little cost that no semantic change has taken
place and incur the minimum change cost. I note here the effectiveness
of GHCI at enabling interactive development under equivalent per-file
reloading conditions as evidence that this model is in fact viable for
enabling the interactive work flow that we associate with Clojure
development.&lt;/p&gt;

&lt;p&gt;With &amp;quot;namespaces&amp;quot; represented as concrete immutable values, we can now
define namespace manipulation operations such as &lt;code&gt;require&lt;/code&gt; and &lt;code&gt;def&lt;/code&gt;
in terms of functions which update the &amp;quot;current&amp;quot; namespace as a first
class value. A &lt;code&gt;def&lt;/code&gt; when evaluated simply takes a namespace and
returns a new namespace that &amp;quot;happens&amp;quot; to contain a new &lt;code&gt;def&lt;/code&gt;. However
the work performed is potentially arbitrary. &lt;code&gt;refer&lt;/code&gt;, the linking part
of &lt;code&gt;require&lt;/code&gt;, can now be implemented as a function which takes some
enumeration of the symbols in some other namespace and the &amp;quot;current&amp;quot;
environment, then returns a &amp;quot;new&amp;quot; environment representing the
&amp;quot;current&amp;quot; environment with the appropriate aliases installed.&lt;/p&gt;

&lt;p&gt;This becomes interesting because it means that the return value of
&lt;code&gt;load&lt;/code&gt; need lot be the &lt;code&gt;eval&lt;/code&gt; result of the last form in the target
file, it can instead be the namespace value representing the final
state of the loaded module. Now, given a caching/memoized &lt;code&gt;load&lt;/code&gt;
(which is &lt;code&gt;require&lt;/code&gt;), we can talk about an &amp;quot;egalitarian&amp;quot; loading
system where user defined loading paths are possible because &lt;code&gt;refer&lt;/code&gt;
only needs the &amp;quot;current&amp;quot; namespace, a &amp;quot;source&amp;quot; namespace and a
spec. Any function could generate a &amp;quot;namespace&amp;quot; value, including one
which happens to perform loading of an arbitrary file as computed by
the user. See &lt;a href=&quot;http://technomancy.us&quot;&gt;technomancy&lt;/a&gt;&amp;#39;s
&lt;a href=&quot;http://p.hagelb.org/egalitarian-ns.html&quot;&gt;egalitarian ns&lt;/a&gt; for enabling
the hosting of multiple versions of a single lib simultaneously in a
single Clojure instance is one possible application of this
behavior.&lt;/p&gt;

&lt;p&gt;It is my hope that by taking this approach the implementation of
namespaces and code loading can be simplified greatly however one
advantage of the &lt;code&gt;Var&lt;/code&gt; structure is that it enables forwards and out
of order declarations which is immensely useful while bootstrapping a
language run time ex nihilo, as done
&lt;a href=&quot;https://github.com/clojure/clojure/blob/master/src/jvm/clojure/lang/RT.java#L179-L233&quot;&gt;here&lt;/a&gt;
in the Clojure core. &lt;code&gt;ns&lt;/code&gt; itself must exist in the &amp;quot;empty&amp;quot; namespace,
otherwise as the &amp;quot;empty&amp;quot; namespace is used to analyze the first form
in a file (stateless (abstractly) compiler ftw) the &lt;code&gt;ns&lt;/code&gt; form itself
will not be resolved and no program can be constructed. Ox could
follow Clojure&amp;#39;s lead and cheat by not implementing &lt;code&gt;ns&lt;/code&gt; in Ox but
rather bootstrapping it from Java or Clojure or whatever the
implementing language turns out to be but I&amp;#39;d like to do better than
that. This is a problem I haven&amp;#39;t solved yet.&lt;/p&gt;

&lt;p&gt;^d&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Compiler introduction of transients to pure functions</title>
    <link href="http://arrdem.com/2014/09/17/compiler_introduction_of_transients_to_pure_functions/"/>
    <updated>2014-09-17T23:13:10+00:00</updated>
    <id>http://arrdem.com/2014/09/17/compiler_introduction_of_transients_to_pure_functions</id>
    <content type="html">&lt;p&gt;In Clojure and pure functinal languages, the abstraction is provided
that values cannot be updated, only new values may be
produced. Naively, this means that every update to a value must
produce a full copy of the original value featuring the desired
change. More sophisticated implementations may opt for structural
sharing, wherein updated versions of some structure share backing
memory with the original or source value on the substructures where no
update is performed. Substructures where there is an update must be
duplicated and updated as in the naive case, but for tree based
datastructures this can reduce the cost of a typical update from
&lt;code&gt;O(N)&lt;/code&gt; on the size of the updated structure to &lt;code&gt;O(log(N))&lt;/code&gt; because the
rest of the structure may be shared and only the &amp;quot;updated&amp;quot; subtree
needs to be duplicated.&lt;/p&gt;

&lt;p&gt;This means that tree based structures which maximize the ammount of
sharable substructure perform better in a functional context because
they minimize the fraction of a datastructure which must be duplicated
during any given update.&lt;/p&gt;

&lt;p&gt;Unfortunately however, such structural sharing still carries a
concrete cost in terms of memory overhead, garbage collection and
cache and performance when compared to a semantically equivalent
update in place over a mutable datastructure. A mutable update is
typically &lt;code&gt;O(1)&lt;/code&gt;, with specific exceptions for datastructures
requiring amortized analysis to achieve near &lt;code&gt;O(1)&lt;/code&gt; performance.&lt;/p&gt;

&lt;p&gt;Ideally, we would be able to write programs such that we preserve the
abstraction of immutable values, while enabling the compiler or other
runtime to detect when intentional updates in place are occurring and
take the opportunity to leverage the performance improvements
consequent from mutable data in these cases while ensuring that no
compiler introduced mutability can become exposed to a user through
the intentional API as built by the programmer.&lt;/p&gt;

&lt;p&gt;In such a &amp;quot;pure&amp;quot; language, there is only one class of functions,
functions from &lt;code&gt;Immutable&lt;/code&gt; values to &lt;code&gt;Immutable&lt;/code&gt; values. However if we
wish to minimize the performance overhead of this model four cases
become obvious. &lt;code&gt;λ Immutable → Immutable&lt;/code&gt; functions are clearly
required as they represent the intentional API that a programmer may
write. &lt;code&gt;λ Mutable → Mutable&lt;/code&gt; functions could be introduced as
implementation details within an &lt;code&gt;λ Immutable → Immutable&lt;/code&gt; block, so
long as the API contract that no mutable objects may leak is
preserved.&lt;/p&gt;

&lt;p&gt;Consider the Clojure program&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Clojure&quot; data-lang=&quot;Clojure&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;reduce &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;partial apply &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;assoc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;vector&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This program will sequentially apply the non-transient association
operation to a value (originally the empty map) until it represents
the identity mapping over the interval [0,9999]. In the naive case,
this would produce 10,000 full single update coppies of the
map. Clojure, thanks to structural sharing, will still produce 10,000
update objects, but as Clojure&amp;#39;s maps are implemented as log₃₂
&lt;a href=&quot;http://en.wikipedia.org/wiki/Hash_array_mapped_trie&quot;&gt;hash array mapped tries&lt;/a&gt;,
meaning that only the array containing the &amp;quot;last&amp;quot; n % 32 key/value
pairs must be duplicated, more the root node. This reduces the cost of
the above operation from &lt;code&gt;T(~10,000²)&lt;/code&gt; to &lt;code&gt;T(10,000*64) ≊ T(640,000)&lt;/code&gt;
which is huge for performance. However, a
&lt;a href=&quot;http://c2.com/cgi/wiki?SufficientlySmartCompiler&quot;&gt;Sufficiently Smart Compiler&lt;/a&gt;
could recognize that the cardinality of the produced map is
&lt;code&gt;max(count(range(10000)), count(range(10000)))&lt;/code&gt;, clearly
being 10000. Consequently an array map of in the worst case 10000
elements is required given ideal hashing, however assuming a load
factor of 2/3 this means our brilliant compiler can preallocate a
hashmap of 15000 entries (presumed &lt;code&gt;T(1)&lt;/code&gt;), and then perform
&lt;code&gt;T(10000)&lt;/code&gt; hash insertions with a very low probability of having to
perform a hash table resize due to accounting for hash distribution
and sizing the allocated table to achieve a set load factor.&lt;/p&gt;

&lt;p&gt;Clearly at least in this example the mutable hash table would be an
immense performance win because while we splurge a bit on consumed
memory due to the hash table load factor (at least compared to my
understanding of Clojure&amp;#39;s hash array mapped trie structure) the
brilliantly compiled program will perform no allocations which it will
not use, will perform no copying, and will generate no garbage
compared to the naive structurally shared implementation which will
produce at least 9,967 garbage pairs of 32 entry arrays.&lt;/p&gt;

&lt;p&gt;The map cardinality hack is it&amp;#39;s own piece of work and may or may not
be compatible with the JVM due to the fact that most structures are
not parametric on initial size and instead perform the traditional 2*n
resizing at least abstractly. However, our brilliant compiler can
deduce that the empty map which we are about to abuse can be used as a
transient and made static when it escapes the scope of the above
expression.&lt;/p&gt;

&lt;p&gt;Consider the single static assignment form for the above (assuming a
reduce definition which macroexpands into a loop) (which Clojure
doesn&amp;#39;t do).&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;    [1 ] = functionRef(clojure.core/partial)
    [2 ] = functionRef(clojure.core/apply)
    [3 ] = functionRef(clojure.core/assoc)
    [4 ] = invoke(2, 3, 4)                   ;; (partial apply assoc)
    [5 ] = {}
    [6 ] = functionRef(clojure.core/map)
    [7 ] = functionRef(clojure.core/vector)
    [8 ] = functionRef(clojure.core/range)
    [9 ] = 10000
    [10] = invoke(8, 9)                      ;; (range 10000)
    [11] = invoke(6, 7, 10, 10)              ;; (map vector [10] [10])
    [12] = functionRef(clojure.core/first)
    [13] = functionRef(clojure.core/rest)
loop:
    [14] = phi(5,  18)
    [15] = phi(11, 19)
    [16] = if(13, cont, end)
cont:
    [17] = invoke(12, 14)
    [18] = invoke(4, 14, 15)
    [19] = invoke(13, 15)
    [20] = jmp(loop)
end:
    [21] = return(18)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Where the phi function represents that the value of the phi node
depends on the source of the flow of control. Here I use the first
argument to the phi functions to mean that control &amp;quot;fell through&amp;quot; from
the preceeding block, and the second argument to mean that control was
returned to this block via instruction 20.&lt;/p&gt;

&lt;p&gt;This representation reveals the dataflow dependence between sequential
values of our victim map. We also have the contract that the return,
above labeled 21, must be of an &lt;code&gt;Immutable&lt;/code&gt; value. Consequently we can
use a trivial dataflow analysis to &amp;quot;push&amp;quot; the &lt;code&gt;Immutable&lt;/code&gt; annotation
back up the flow graph, giving us that 18, 14 and 5 must be immutable,
5 is trivially immutable, 18 depends on 14, which depends on 18 and 5,
implying that it must be immutable as well. So far so good.&lt;/p&gt;

&lt;p&gt;We can now recognize that we have a &lt;code&gt;phi(Immutable, Immutable)&lt;/code&gt; on a
loop back edge, meaning that we are performing an update of some sort
within the loop body. This means that, so long as &lt;code&gt;Transient&lt;/code&gt; value is
introduced into the &lt;code&gt;Immutable&lt;/code&gt; result, we can safely rewrite the
&lt;code&gt;Immutable&lt;/code&gt; result to be a &lt;code&gt;Transient&lt;/code&gt;, and add a &lt;code&gt;persistent!&lt;/code&gt;
invocation before the return operation. Now we have &lt;code&gt;phi(Immutable,
Transient) → Transient&lt;/code&gt; which makes no sense, so we add a loop header
entry to make the initial empty map &lt;code&gt;Transient&lt;/code&gt; giving us
&lt;code&gt;phi(Transient, Transient) → Transient&lt;/code&gt; which is exactly what we
want. Now we can rewrite the loop update body to use
&lt;code&gt;assoc! → Transient Map → Immutable Object → Immutable Object → Transient Map&lt;/code&gt;
rather than
&lt;code&gt;assoc → Immutable Map → Immutable Object → Immutable Object → Immutable Map&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Note that I have simplified the signature of assoc to the single
key/value case for this example, and that the key and value must both
be immutable. This is required as the &lt;code&gt;persistent!&lt;/code&gt; function will
render only the target object itself and not its references
persistent.&lt;/p&gt;

&lt;p&gt;This gives us the final operation sequence&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;    [1 ] = functionRef(clojure.core/partial)
    [2 ] = functionRef(clojure.core/apply)
    [3 ] = functionRef(clojure.core/assoc!)
    [4 ] = invoke(2, 3, 4)                      ;; (partial apply assoc)
    [5 ] = functionRef(clojure.core/transient!)
    [6 ] = {}
    [7 ] = invoke(5, 6)
    [8 ] = functionRef(clojure.core/map)
    [9 ] = functionRef(clojure.core/vector)
    [10] = functionRef(clojure.core/range)
    [11] = 10000
    [12] = invoke(10, 11)                       ;; (range 10000)
    [13] = invoke(8, 9, 12, 12)                 ;; (map vector [12] [12])
    [14] = functionRef(clojure.core/first)
    [15] = functionRef(clojure.core/rest)
loop:
    [16] = phi(7,  20)
    [17] = phi(13, 21)
    [18] = if(17, cont, end)
cont:
    [19] = invoke(14, 17)
    [20] = invoke(4, 16, 17)
    [21] = invoke(15, 17)
    [22] = jmp(loop)
end:
    [23] = functionRef(clojure.core/persistent!)
    [24] = invoke(23, 20)
    [25] = return(24)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Having performed this rewrite we&amp;#39;ve one. This transform allows an
arbitrary loop using a one or more persistent datastructures a
accumulators to be rewritten in terms of transients if there exists
(or can be inferred) a matching &lt;code&gt;Transient t → Transient t&lt;/code&gt; updater
equivalent to the updater used. Note that if a non-standard library
updater (say a composite updater) is used, then the updater needs to
be duplicated and if possible recursively rewritten from a &lt;code&gt;Persistent
t → Persistent t&lt;/code&gt; by replacing the standard library operations for
which there are known doubles with their &lt;code&gt;Transient t&lt;/code&gt; counterparts
until either the rewrite fails to produce a matching &lt;code&gt;Transient t →
Transient t&lt;/code&gt; or succeeds. If any such rewrite fails then this entire
transform must fail. Also note that this transformation can be applied
to subterms... so as long as the &lt;code&gt;Persistent t&lt;/code&gt; contract is not
violated on the keys and values here of the map in a nontrivial
example their computation as well could be rewritten to use compiler
persisted transients.&lt;/p&gt;

&lt;p&gt;Now yes you could have just written&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-Clojure&quot; data-lang=&quot;Clojure&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;into &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;vector&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;which would have used transients implicitly, but that requires that
the programmer manually perform an optimization requiring further
knowledge of the language and its implementation details when clearly
a relatively simple transformation would not only reveal the potential
for this rewrite but would provide it in the general case of
arbitrarily many mutable accumulators rather than into&amp;#39;s special case
of just one.&lt;/p&gt;

&lt;p&gt;^d&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>牛: a preface</title>
    <link href="http://arrdem.com/2014/09/10/ox:_a_preface/"/>
    <updated>2014-09-10T17:09:39+00:00</updated>
    <id>http://arrdem.com/2014/09/10/ox:_a_preface</id>
    <content type="html">&lt;p&gt;&lt;a href=&quot;https://github.com/oxlang/oxlang&quot;&gt;牛&lt;/a&gt; (Ox or oxlang) is an
experiment, a dream, a thought which I can&amp;#39;t seem to get out of my
head. After working primarily in
&lt;a href=&quot;https://twitter.com/richhickey&quot;&gt;Rich Hicky&lt;/a&gt;&amp;#39;s excelent language
&lt;a href=&quot;https://clojure.org&quot;&gt;Clojure&lt;/a&gt; for over two years now and spending a
summer hacking on &lt;a href=&quot;https://github.com/oxlang/oxcart&quot;&gt;Oxcart&lt;/a&gt;, an
optimizing compiler for Clojure, it is my concidered oppinion that
Clojure is a language which Rich invented for his own productivity at
great personal effort. As such, Clojure is a highly oppinionated Lisp,
which makes some design decisions and has priorities.&lt;/p&gt;

&lt;p&gt;Ultimately I think that the Clojure language is underspecified. There
is no formal parse grammar for Clojure&amp;#39;s tokens. There is no spec for
the Clojure standard library. Due to underspecification Clojure as a
language is bound to the Rich&amp;#39;s implementation for Java as due to
leaking implementation details and lack of a spec no other
implemetation can ensure compatability. Even ClojureScript, the
official effort to implement Clojure atop Javascript is at best a
dialect due to the huge implementation differences between the two
languages.&lt;/p&gt;

&lt;p&gt;These are not to say that Clojure is a bad language. I think that
Clojure is an excellent language. If there is an Ox prototype, it will
likely be built in Clojure. It&amp;#39;s just that my priorities and Rich&amp;#39;s
are at a mismatch. Rich is it seems happy with the existing JVM
implementation of Clojure, and I see that there&amp;#39;s a lot of really
interesting work that could be done to optimize, type and statically
link Clojure or a very Clojure like language and that such work is
very unlikely to become part of the Clojure core.&lt;/p&gt;

&lt;p&gt;Clojure&amp;#39;s lack of specification makes it futile for me to invest in
providing an imperfect implementation so the only thing that makes
sense to do is specify my own lang.  The result is the Ox
language. The joke in the name is twofold: the compiler for Clojure I
was working on when I originally had the idea for Ox was Oxcart, named
after the A-12/SR-71 program. The rest of it is in the name. Oxen are
slow, quiet, tractable beasts of burden used by many cultures. This
ultimately characterizes the language which I&amp;#39;m after in the Ox
project. There&amp;#39;s also some snarking to be found about the tractability
of the Ox compared to that of other languages mascots like the gopher,
the camel and the gnu which are much less willing.&lt;/p&gt;

&lt;p&gt;The hope of the Ox project is to produce a mostly typed, mostly pure
language which is sufficiently well specified that it can support both
static and dynamic implementations on a variety of platforms. Ox draws
heavily on my experience with Clojure&amp;#39;s various pitfalls, as well as
my exposure to Haskell, Shen, Kiss and Ocaml to produce a much more
static much more sound Lisp dialect language which specifies a
concrete standard library and host interoperation mechanics in a
platform abstract way amenable to both static and dynamic
implementations on a variety of platforms.&lt;/p&gt;

&lt;p&gt;In the forthcomming posts I will attempt to illustrate the model and
flavor of the Ox programming language, as well as sketch at some
implementation details that differentiate Ox from Clojure and I think
make it a compelling project. Note however that as of now Ox is and
will remain vaporware. As with the Kernel lisp project, while I may
choose to implement it eventually the exercise of writing these posts
is really one of exploring the Lisp design space and trying to
determine for myself what merit this project has over the existing
languages from which it draws inspiration.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/2014/10/27/ox:_the_environment_model/&quot;&gt;The Oxlang environment model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;^d&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>On Future Languages</title>
    <link href="http://arrdem.com/2014/08/26/on_future_languages/"/>
    <updated>2014-08-26T05:05:56+00:00</updated>
    <id>http://arrdem.com/2014/08/26/on_future_languages</id>
    <content type="html">&lt;p&gt;As &lt;a href=&quot;https://twitter.com/extempore2&quot;&gt;Paul Philips&lt;/a&gt; notes at the end of
his talk
&lt;a href=&quot;https://www.youtube.com/watch?v=TS1lpKBMkgg&quot;&gt;We&amp;#39;re Doing It All Wrong [2013]&lt;/a&gt;,
ultimately a programming language is incidental to building software
rather than critical. Ultimately the &amp;quot;software developer&amp;quot; industry is
not paid to write microcode. Rather, software as an industry exists to
deliver buisness solutions. Similarly computers themselves are by a
large incidental. Rather, they are agents for delivering data
warehousing, search and transmission solutions. Very few people are
employed to improve software and computer technology for the sake of
doing so compared to the hordes of industry programmers who ultimately
seek to use computers as a magic loom to create value for a
business. In this light I think it&amp;#39;s meaningful to investigate recent
trends in programming languages and software design as they pertain to
solving problems not to writing code.&lt;/p&gt;

&lt;p&gt;As the last four or five decades of writing software stand witness,
software development is rarely an exercise in first constructing a
perfect program, subsequently delivering it and watching pleased as a
client makes productive use of it until the heat death of the
universe. Rather, we see that software solutions when delivered are
discovered to have flaws, or didn&amp;#39;t solve the same problem that the
client thought that it would solve, or just didn&amp;#39;t do it quite right,
or the problem changed. All of these changes to the environment in
which the software exists demand changes to the software itself.&lt;/p&gt;

&lt;h2&gt;Malleability&lt;/h2&gt;

&lt;p&gt;In the past, we have attempted to develop software as if we were going
to deploy perfect products forged of pure adamantium which will endure
forever unchanged. This begot the entire field of software
architecture and the top down school of software design. The issue
with this model as the history of top down software engineering stands
testament is that business requirements change if they are known, and
must be discovered and quantified if they are unknown. This is an old
problem
&lt;a href=&quot;http://users.ece.utexas.edu/%7Eperry/education/SE-Intro/fakeit.pdf&quot;&gt;with no good solution&lt;/a&gt;.
In the face of incomplete and/or changing requirements all that can be
done is to evolve software as rapidly and efficiently as possible to
meet changes as Parnas argues.&lt;/p&gt;

&lt;p&gt;In the context of &lt;em&gt;expecting&lt;/em&gt; change, languages and the other tools
used to develop changing software must be efficient to re-architect
and change. As Paul Philips says in the above talk,
&lt;a href=&quot;https://www.youtube.com/watch?feature=player_detailpage&amp;amp;v=TS1lpKBMkgg#t=307&quot;&gt;&amp;quot;modification is undesirable, modifiability is paramount&amp;quot;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Looking at languages which seem to have enjoyed traction in my
lifetime, the trend seems to have been that, with the exception of
tasks for which the language in which the solution was to be built was
a requirement the pendulum both of language design and of language use
has been swinging away from statically compiled languages like Java, C
&amp;amp; C++ (the Algol family) towards interpreted languages (Perl, Python,
Ruby, Javascript) which trade off some performance for interactive
development and immediacy of feedback.&lt;/p&gt;

&lt;p&gt;Today, that trend would seem to be swinging the other way. Scala, a
statically checked language base around extensive type inference with
some interactive development support has been making headway. Java and
C++ seem to have stagnated but are by no means dead and gone. Google
Go, Mozilla Rust, Apple Swift and others have appeared on the scene
also fitting into this intermediary range between interactive and
statically compiled with varying styles of type inference to achieve
static typing while reducing programmer load. Meanwhile the hot
frontier in language research seems to be static typing and type
inference as the liveliness of the Haskell ecosystem is ample proof.&lt;/p&gt;

&lt;p&gt;Just looking at these two trends, I think it&amp;#39;s reasonable to draw the
conclusion that interpreted, dynamically checked, dynamically
dispatched languages like Python and Ruby succeeded at providing more
malleable programming environments than the languages which came
before them (the Algol family &amp;amp; co). However while making changes in a
dynamically checked language is well supported, maintaining
correctness is difficult because there is no compiler or type checker
to warn you that you&amp;#39;ve broken something. This limits the utility of
malleable environments, because software which crashes or gives
garbage results is of no value compared to software which behaves
correctly. However, as previously argued, software is not some work of
divine inspiration which springs fully formed from the mind onto the
screen. Rather the development of software is an evolutionary
undertaking involving revision (which is well suited to static type
checking) and discovery which may not be.&lt;/p&gt;

&lt;p&gt;As a checked program must always be in a legal (correct with respect
to the type system) state by definition, this precludes some elements
of development by trial and error as the type system will ultimately
constrain the flexibility of the program requiring systematic
restructuring where isolated change could have sufficed. This is not
argued to be a flaw, it is simply a trade off which I note between
allowing users to express and experiment with globally &amp;quot;silly&amp;quot; or
&amp;quot;erroneous&amp;quot; constructs and the strict requirement that all programs be
well formed and typed when with respect to some context or the
programmer&amp;#39;s intent the program may in fact be well formed.&lt;/p&gt;

&lt;p&gt;As program correctness is an interesting property, and one which
static model checking including &amp;quot;type systems&amp;quot; is well suited to
assisting with, I do not mean to discount typed languages. Ultimately,
a correct program must be well typed with respect to some type system
whether that system is formalized or not.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Program testing can be used to show the presence of bugs, but never
to show their absence!&lt;/p&gt;

&lt;p&gt;~ Dijkstra (1970)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Static model checking on the other hand, can prove the presence of
flaws with respect to some system. This property alone makes static
model checking an indispensable part of software assurance as it
cannot be replaced by any other non-proof based methodology such as
assertion contracts or test cases.&lt;/p&gt;

&lt;p&gt;Given this apparent trade off between flexibility and correctness,
Typed Racket, Typed Clojure and the recent efforts at Typed Python are
interesting, because they provide halfway houses between the &amp;quot;wild
west&amp;quot; of dynamic dispatch &amp;amp; dynamic checking languages like
traditional Python, Ruby and Perl and the eminently valuable model
checking of statically typed languages. This is because they enable
programmers to evolve a dynamically checked system, passing through
states of varying levels of soundness towards a better system and then
once it has reached a point of stability solidify it with static
typing, property based testing and other static reasoning techniques
without translating programs to another language which features
stronger static analysis properties.&lt;/p&gt;

&lt;h2&gt;Utility &amp;amp; Rapidity from Library Support&lt;/h2&gt;

&lt;p&gt;Related to malleability in terms of ultimately delivering a solution
to a problem that gets you paid is the ability to get something done
in the first place. Gone (forever I expect) are the days when programs
are built without using external libraries. Looking at recent
languages, package/artifact management and tooling capable of
trivializing leveraging open source software has EXPLODED. Java has
&lt;code&gt;mvn&lt;/code&gt;, Python has &lt;code&gt;pip&lt;/code&gt;, Ruby has &lt;code&gt;gem&lt;/code&gt;, Haskell has &lt;code&gt;cabal&lt;/code&gt;, Node has
&lt;code&gt;npm&lt;/code&gt; and the Mozilla Rust team deemed a package manager so critical
to the long term success of the language that they built their &lt;code&gt;cargo&lt;/code&gt;
system long before the first official release or even release
candidate of the language.&lt;/p&gt;

&lt;p&gt;Why are package managers and library infrastructure critical? Because
they enable massive code reuse, &lt;em&gt;especially of vendor code&lt;/em&gt;. Building
a webapp? Need a datastore? The investment of buying into any
proprietary database you may choose has been driven so low by the ease
with which $free (and sometimes even free as in freedom) official
drivers can be found and slotted into place it&amp;#39;s silly. The same goes
for less vendor specific code... regex libraries, logic engine
libraries, graphics libraries and many more exist in previously
undreamed of abundance (for better or worse) today.&lt;/p&gt;

&lt;p&gt;The XKCD &lt;a href=&quot;http://gkoberger.github.io/stacksort/&quot;&gt;stacksort&lt;/a&gt; algorithm
is a tongue in cheek reference to the sheer volume of free as in
freedom forget free as in beer code which can be found and leveraged
in developing software today.&lt;/p&gt;

&lt;p&gt;This doesn&amp;#39;t just go for &amp;quot;library&amp;quot; support, I&amp;#39;ll also include here FFI
support. Java, the JVM family of languages, Haskell, Ocaml and many
others gain much broader applicability for having FFI interfaces for
leveraging the decades of C and C++ libraries which predate
them. Similarly Clojure, Scala and the other &amp;quot;modern&amp;quot; crop of JVM
languages gain huge utility and library improvements from being able
to reach through to Java and leverage the entire Java ecosystem
selectively when appropriate.&lt;/p&gt;

&lt;p&gt;While it&amp;#39;s arguably unfair to compare languages on the basis of the
quantity of libraries available as this metric neglects functionally
immeasurable quality and utility, the presence of &lt;em&gt;any&lt;/em&gt; libraries is a
legitimate comparison in terms of potential productivity to
comparative absence.&lt;/p&gt;

&lt;p&gt;What good is a general purpose building material, capable of
constructing any manner of machine, when simple machines such as the
wheel or the ramp must be reconstructed by every programmer? Not
nearly so much utility as a building material providing these things
on a reusable basis at little or no effort to the builder regardless
of the ease with which one may custom build such tools as needed.&lt;/p&gt;

&lt;h2&gt;So What&amp;#39;s the Big Deal&lt;/h2&gt;

&lt;p&gt;I look at this and expect to see two trends coming out of it. The
first of which is that languages with limited interoperability and/or
limited library bases are dead. Stone cold dead. Scheme, RⁿRS, and
Common Lisp were my introductions to the Lisp family of
languages. They are arguably elegant and powerful tools, however
compared to other tools such as python they seem offer at best equal
leverage due to prevailing lack of user let alone newbie friendly
library support compared to other available languages.&lt;/p&gt;

&lt;p&gt;I have personally written 32KLoC in Clojure. More counting
intermediary diffs. That I can find on my laptop. Why? Because Clojure
unlike my experiences with Common Lisp and Scheme escapes the
proverbial
&lt;a href=&quot;http://www.winestockwebdesign.com/Essays/Lisp_Curse.html&quot;&gt;lisp curse&lt;/a&gt;
simply thanks to tooling which facilitates library and infrastructure
sharing at a lower cost than the cost of reinvention. Reinvention
still occurs, as it always will, but the marginal cost of improving an
existing tool vs writing a new one is in my experience a compelling
motivator for maintaining existing tool kits and software. This means
that Clojure at least seems to have broken free of the black hole of
perpetual reinvention and is consequently liberating to attack &lt;em&gt;real&lt;/em&gt;
application critical problems rather than distracting programmers into
fighting simply to build a suitable environment.&lt;/p&gt;

&lt;p&gt;It&amp;#39;s not that Clojure is somehow a better language, arguably it
ultimately isn&amp;#39;t since it lacks the inbuilt facilities for many
interesting static proof techniques, but that&amp;#39;s not the point. As
argued above, the language(s) we use are ultimately incidental to the
task of building a solution to some problem. What I really want is
leverage from libraries, flexibility in development, optional and/or
incremental type systems and good tooling. At this task, Clojure seems
to be a superior language.&lt;/p&gt;

&lt;h2&gt;The Long View&lt;/h2&gt;

&lt;p&gt;This is not all to say that I think writing languages is pointless,
nor that the languages we have today are the best we&amp;#39;ve ever had let
alone the best we ever will have at simultaneously providing utility,
malleability and safety. Nor is this to say that we&amp;#39;ll be on the JVM
forever due to the value of legacy libraries or something equally
silly. This is however to say that I look with doubt upon language
projects which do not have the benefit of support from a &amp;quot;major
player&amp;quot;, a research entity or some other group willing to fund long
term development in spite of short term futility simply because the
literal price of bootstrapping a &amp;quot;new&amp;quot; language into a state of
compelling utility is expensive in terms of man-years.&lt;/p&gt;

&lt;p&gt;This conclusion is, arguably, my biggest stumbling block with my
&lt;a href=&quot;http://github.com/oxlang/oxlang&quot;&gt;Oxlang&lt;/a&gt; project. It&amp;#39;s not that the
idea of the language is bad, it&amp;#39;s that a tradeoff must be carefully
made between novelty and utility. Change too much and Oxlang will be
isolated from the rest of the Clojure ecosystem and will loose hugely
in terms of libraries as a result. Change to little and it won&amp;#39;t be
interesting compared to Clojure. Go far enough, and it will cross the
borders of hard static typing, entering the land of Shen, Ocaml and
Haskell and as argued above I think sacrifice interesting flexibility
for uncertain gains.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>On Student Startups</title>
    <link href="http://arrdem.com/2014/08/10/on_student_startups/"/>
    <updated>2014-08-10T21:28:06+00:00</updated>
    <id>http://arrdem.com/2014/08/10/on_student_startups</id>
    <content type="html">&lt;p&gt;When I enrolled in UT Austin&amp;#39;s &amp;quot;student startup semenar&amp;quot; one of the
guest speaker comments which stood out to me and has stuck most firmly
in my mind is that &amp;quot;there are no new ideas, only good execution&amp;quot;. This
particular lecturer described how he kept a notebook full of random
ideas he had for possible businesses, and talked at length about the
importance of validating business models through surveys of potential
customers as well as discussions with industry peers. The takeaway he
left us with was that consequently rather than attempting to operate
in &amp;quot;stealth&amp;quot; mode as seems to be fashionable for so many startups
developing a product, he argued that ideas are so cheap and the first
mover advantage so great due to simple startup execution costs that
attempting to cloak a startup&amp;#39;s model and/or product generated no
measurable advantage and had a concrete cost in terms of potential
comment from consumers and peers which is lost as a consequence of
secrecy.&lt;/p&gt;

&lt;p&gt;Of the dozen or so startups I&amp;#39;ve interacted with so far, both in and
outside the context of the abovementioned startup seminar, I&amp;#39;ve seen
this overvaluing of secrecy over and over again, especially when
requesting feedback on an idea. On Freenode&amp;#39;s #Clojure channel we have
standing joke: the &amp;quot;ask to ask&amp;quot; protocol. Under the ask to ask
protocol, some first timer will join the channel and ask if anyone
knows about some tool X whereupon some longtime denizen will invoke
the ask to ask protocol and tell the newcomer to just ask his real
question.&lt;/p&gt;

&lt;p&gt;When I see a request from a nontechnical founder for technical
feedback over a coffee date or in a private context after an NDA, all
I can think of is the ask to ask protocol and the litany against
startup secrecy. A coffee date is a commitment of at least an hour of
what would otherwise been paid consulting time, and an NDA is a
legally binding commitment. For the privilage of signing a contract
and giving advice I get what... coffee? A mention in the credits when
you finally &amp;quot;break stealth&amp;quot;?  What if I was nursing an equivalent
idea? I can&amp;#39;t know that until &lt;em&gt;after&lt;/em&gt; I sign your silly NDA, which is
kinda a problem because now you&amp;#39;ve robbed me of the ability to
capitalize on my own prior art.&lt;/p&gt;

&lt;p&gt;An email or a forum comment is free. By asking that an engineer go on
a coffee date to hear a pitch let alone sign an NDA and then comment,
the petitioner (see nontechnical founder) is entirely guilty of at
best asking to ask and limiting themselves to one or two responses
when several could have been had were the real question posed rather
than an ask to ask instance. Start talking about NDAs and I expect
you&amp;#39;ll get what you pay for.&lt;/p&gt;

&lt;p&gt;^d&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Of Oxen, Carts and Ordering</title>
    <link href="http://arrdem.com/2014/08/06/of_oxen,_carts_and_ordering/"/>
    <updated>2014-08-06T01:40:32+00:00</updated>
    <id>http://arrdem.com/2014/08/06/of_oxen,_carts_and_ordering</id>
    <content type="html">&lt;p&gt;Well, the end of
&lt;a href=&quot;https://developers.google.com/open-source/soc/?csw=1&quot;&gt;Google Summer of Code&lt;/a&gt;
is in sight and it&amp;#39;s long past time for me to make a report on the
state of my Oxcart Clojure compiler
project. &lt;a href=&quot;/2014/06/26/oxcart_and_clojure/&quot;&gt;When last I wrote about it&lt;/a&gt;,
Oxcart was an analysis suite and a work proposal for an emitter.&lt;/p&gt;

&lt;p&gt;To recap the previous post: Clojure uses Var objects, an atomic,
threadsafe reference type with support for naming and namespace
binding semantics to create a literal global hashmap from symbols to
binding vars, with a literal stack of thread local bindings. These
vars form the fundamental indirection mechanism by which Clojure
programs map from textual symbols to runtime functions. Being
atomically mutable, they also serve to enable dynamic re-binding and
consequently enable REPL driven development.&lt;/p&gt;

&lt;p&gt;But for this second aspect of providing for dynamic redefinition of
symbols, Clojure could be statically compiled eliminating var
indirection and achieving a performance improvement.&lt;/p&gt;

&lt;p&gt;Moreover, in the style of Clojurescript, exposing the full source of
the language to an agressive static compiler could yield total program
size improvements in comparison to programs running on the official
Clojure compiler/runtime pair.&lt;/p&gt;

&lt;p&gt;So. This was the premise upon which my Project Oxcart GSoC began. Now,
standing near the end of GSoC what all has happened, where does the
project stand and what do I consider results?&lt;/p&gt;

&lt;p&gt;As of this post&amp;#39;s writing,
&lt;a href=&quot;https://github.com/arrdem/oxcart/commit/e7a22a091cfb7188280c89f7d185aec10bdba6cd&quot;&gt;e7a22a09&lt;/a&gt;
is the current state of the Oxcart project. The Oxcart loader and
analyzer, built atop Nicola Mometto&amp;#39;s tools.analyzer.jvm and
tools.emitter.jvm, is capable of loading and generating a whole
program AST for arbitrary Clojure progrms. The various passes in the
&lt;code&gt;oxcart.passes&lt;/code&gt; subsystem implement a variety of per-form and whole
program traversals including λ lifting, use analysis, reach analysis
and taken as value analysis. There is also some work on a multiple
arity function reduction system, but that seems to be a problem module
at present. The &lt;code&gt;oxcart.emitter&lt;/code&gt; subsystem currently features two
emitters, only one of which I can claim is of my
authoring. &lt;code&gt;oxcart.emitter.clj&lt;/code&gt; is a function from an Oxcart whole
program AST to a &lt;code&gt;(do)&lt;/code&gt; form containing source code for the entire
program as loaded. This has primarily been a tool for debugging and
ensuring the sanity of various program transformations.&lt;/p&gt;

&lt;p&gt;The meat of Oxcart is &lt;code&gt;oxcart.emitter.jvm&lt;/code&gt;, a wrapper around a
modified version of &lt;code&gt;Clojure.tools.jvm.emit&lt;/code&gt; which features changes
for emitting statically specialized bytecode which doesn&amp;#39;t make use of
var indirection. These changes are new, untested and subject to change
but they seem to work. As evidenced by the &lt;code&gt;bench-vars.sh&lt;/code&gt; script in
the Oxcart project&amp;#39;s root directory, for some programs the static
target linking transform done by Oxcart can achieve a 24% speedup.&lt;/p&gt;

&lt;blockquote&gt;
&lt;pre&gt;
Times in ms.
Running Clojure 1.6.0 compiled test.vars....
1603.894357
1369.60414
1348.747718
1345.55669
1343.380462
1341.73058
1346.342237
1345.655224
1395.983262
1340.063011
1339.7823
1399.282023
1451.58462
1353.231654
1348.458151
Oxcart compiling test.vars....
Running Oxcart compiled test.vars....
1256.391888
1066.860551
1033.764971
1031.380052
1032.09911
1030.032666
1040.505754
1040.419533
1041.749353
1040.038301
1041.699772
1044.441135
1095.809551
1072.494466
1047.65782
&lt;/pre&gt;
&lt;/blockquote&gt;

&lt;p&gt;How/why is this possible? The benchmark above is highly artificial in
that it takes 500 defs, and tests over several thousand iterations of
selectively executing half of them at random. This benchmark is
designed to exaggerate the runtime cost of var indirection by being
inefficient to inline and making great use of the (comparatively)
expensive var dereference operation.&lt;/p&gt;

&lt;p&gt;So what does this mean for Clojure? Is Oxcart proof that we can and
should build a faster Clojure implementation or is there a grander
result here we should consider?&lt;/p&gt;

&lt;p&gt;Clojure&amp;#39;s existing compiler operates on a &amp;quot;good enough&amp;quot; principle.
It&amp;#39;s not especially nice to read nor especially intelligent, but it
manages to produce reasonably efficient bytecode. The most important
detail of the reference Clojure compiler is that it operates on a form
by form basis and is designed to do so. This is an often forgotten
detail of Clojure, and one which this project has made me come to
appreciate a lot more.&lt;/p&gt;

&lt;p&gt;When is static compilation appropriate and valuable? Compilation is
fundamentally an analysis and specialization operation designed to
make a trade off between &amp;quot;start&amp;quot; or &amp;quot;compile&amp;quot; time and complexity and
runtime performance. This suggests that in different contexts
different trade offs may be appropriate. Furthermore, static
compilation tends to inhibit program change. To take the extreme
example of say C code which is directly linked change in a single
function, if a single function increases in bytecode size so that it
cannot be updated in place. In this case all code which makes use of
it (worst case the rest of the program) must be rewritten to reflect
the changed location of the changed function. While it is possible to
build selective recompilation systems which can do intelligent and
selective rebuilding (GHCI being an example of this), achieving full
compilation performance at interactive development time is simply a
waste of time on compilation when the goal of REPL driven development
is to provide rapid feedback and enable exploration driven development
and problem solving.&lt;/p&gt;

&lt;p&gt;While vars are clearly inefficient from a perspective of minimizing
function call costs, they are arguably optimal in terms of enabling
this development style. Consider the change impact of altering a
single definition on a Clojure instance using var indirection rather
than static linking. There&amp;#39;s no need to compute, recompile and reload
the subset of your program impacted by this single change. Rather the
single changed form is recomputed, and the var(s) bound by the
recompiled expression are altered. In doing so, no changes need be
made to the live state of the rest of the program. When next client
code is invoked the JVM&amp;#39;s fast path if any will be invalidated as the
call target has changed, but this is handled silently by the runtime
rather than being a language implementation detail. Furthermore var
indirection means that compiling an arbitrary Clojure form is
trivial. After handling all form local bindings (let forms and
soforth), try to resolve the remaining expressions to vars in the
mapped namespace, and to a class if no var is found. Brain dead even,
but sufficient and highly effective in spite of its lack of
sophistication.&lt;/p&gt;

&lt;p&gt;While, as Oxcart is proof, it is possible to build a static compiler
for some subset of Clojure, doing so produces not a static Clojure but
a different language entirely because so much of the Clojure ecosystem
and standard library even are defined in terms of dynamic
redefinition, rebinding and dispatch. Consider for instance the
multimethod system, often lauded with the claim that multimethod code
is user extensible. Inspecting the macorexpand of a &lt;code&gt;defmethod&lt;/code&gt; form
you discover that its implementation far from being declarative is
based on altering the root binding of the multimethod to install a new
dispatch value. While it would be possible to statically compile this
&amp;quot;feature&amp;quot; by simply collecting all defmethods over each multimethod
and AOT computing the final dispatch table, however this is semantics
breaking as it is strictly legal in Clojure to dynamically define
additional multimethod entries just as it is legal to have an
arbitrary do block containing defs.&lt;/p&gt;

&lt;p&gt;So, in short, by trying to do serious static compilation you largely
sacrifice REPL development and to do static compilation of Clojure you
wind up defining another language which is declarative on defs,
namespaces and dependencies rather than imperative as Clojure is. I
happen to think that such a near-Clojure static language, call it
Clojurescript on the JVM, would be very interesting and valuable but
Clojure as implemented currently on the JVM is no such thing. This
leads me to the relatively inescapable conclusion that building a
static compiler for Clojure is putting the cart before the ox since
the language simply was not designed to benefit from it.&lt;/p&gt;

&lt;h3&gt;Moving Forwards&lt;/h3&gt;

&lt;p&gt;Now. Does this mean we should write off tools.emitter.jvm, Clojure in
Clojure and Oxcart? By no means. tools.emitter.jvm uses the same var
binding structure and function interface that Clojure does. Moreover
it&amp;#39;s a much nicer and more flexible single form level compiler that I
happen to think represents a viable and more transparent replacement
for &lt;code&gt;Clojure.lang.Compiler&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So what&amp;#39;s that leave on the table? Open question. Clojure&amp;#39;s compiler
has only taken major changes from two men: Rich and Stu. While there
is merit in considered design and stability, this also means that
effort towards cleaning up the core of Clojure itself and not directed
at a hard fork or redesign of Clojure is probably wasted especially in
the compiler.&lt;/p&gt;

&lt;p&gt;Clearly while var elimination was a successful optimization due to the
value Clojure derives from the Var system it&amp;#39;s not a generally
applicable one. However it looks like Rich has dug the old
invokeStatic code back out for Clojure 1.7 and the grapevine is making
10% noises, which is on par with what Oxcart seems to get for more
reasonable inputs so we&amp;#39;ll see where that goes.&lt;/p&gt;

&lt;p&gt;While immutable datastructures are an awesome abstraction, Intel, AMD
and ARM have gotten very good at building machines capable of
exploiting program locality for performance and this property is
fundamentally incompatible with literal immutability. Transients can
help mitigate Clojure&amp;#39;s memory woes, and compiler introduction of
transients to improve memory performance could be
interesting. Unfortunately again this is a whole program optimization
which clashes with the previous statement of the power that Clojure
derives from single form compilation.&lt;/p&gt;

&lt;p&gt;Using core.typed to push type signatures down to the bytecode level
would be interesting, except that since almost everything in Clojure
is a boxed object and object checkcasts are cheap and this would
probably result in little performance improvement unless the core
datatypes were reworked to be type parametric. Also another whole
program level transform requiring an Oxcart style whole program
analysis system.&lt;/p&gt;

&lt;p&gt;The most likely avenue of work is that I&amp;#39;ll start playing with a
partial fork of Clojure which disassociates RT from the compiler,
&lt;code&gt;data_readers.clj&lt;/code&gt;, &lt;code&gt;user.clj&lt;/code&gt; and &lt;code&gt;Clojure/core.clj&lt;/code&gt;. While this will
render Oxcart even more incompatible with core Clojure, it will also
free Oxcart to emit Clojure.core as if it were any other normal
Clojure code including tree shaking and escape the load time overhead
of bootstrapping Clojure core entirely.&lt;/p&gt;

&lt;p&gt;This coming Monday I&amp;#39;ll be giving a talk on Oxcart at the Austin TX
Clojure meetup, so there&amp;#39;s definitely still more to come here
regardless of the Clojure/Conj 14 accepted talk results.&lt;/p&gt;

&lt;p&gt;^D&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Of Mages and Grimoires</title>
    <link href="http://arrdem.com/2014/07/12/of_mages_and_grimoires/"/>
    <updated>2014-07-12T04:43:40+00:00</updated>
    <id>http://arrdem.com/2014/07/12/of_mages_and_grimoires</id>
    <content type="html">&lt;p&gt;When I first got started with Clojure, I didn&amp;#39;t know (and it was a
while before I was told) about the &lt;code&gt;Clojure.repl&lt;/code&gt; toolkit which offers
Clojure documentation access from within an nREPL instance. Coming
from the Python community I assumed that Clojure, like Python, has
excellent API documentation with examples that a total n00b like
myself could leverage to bootstrap my way into simple competence.&lt;/p&gt;

&lt;p&gt;While Clojure does indeed have web documentation hosted on GitHub&amp;#39;s
Pages service, they are eclipsed in Google PageRank score if not in
quality as well by a community built site owned by Zachary Kim:
&lt;a href=&quot;http://Clojuredocs.org&quot;&gt;ClojureDocs&lt;/a&gt;. This wouldn&amp;#39;t be a real problem
at all, were it not for the fact that ClojureDocs was last updated
when Clojure 1.3 was released. In 2011.&lt;/p&gt;

&lt;p&gt;While Clojure&amp;#39;s core documentation and core toolkits haven&amp;#39;t changed
much since the 1.3 removal of &lt;code&gt;Clojure.contrib.*&lt;/code&gt;, I have recently
felt frustrated that newer features of Clojure such as the &lt;code&gt;as-&amp;gt;&lt;/code&gt;, and
&lt;code&gt;cond-&amp;gt;&amp;gt;&lt;/code&gt; which I find very useful in my day to day Clojure hacking
not only were impossible to search for on Google (being made of
characters Google tends to ignore) but also didn&amp;#39;t have ClojureDocs
pages due to being additions since 1.3. Long story short: I finally
got hacked off enough to yak shave my own alternative.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;**drumroll**&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;center mardown=&quot;1&quot;&gt;
&lt;a href=&quot;http://grimoire.arrdem.com/&quot;&gt;Grimoire&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2&gt;Mission&lt;/h2&gt;

&lt;p&gt;Grimoire seeks to do what ClojureDocs did, being provide community
examples of Clojure&amp;#39;s core functions along with their source code and
official documentation. However with Grimoire I hope to go farther
than ClojureDocs did in a number of ways.&lt;/p&gt;

&lt;p&gt;I would like to explore how I find and choose the functions I need,
and try to optimize accessing Grimoire accordingly so that I can find
the right spanner as quickly as possible. Part of this effort is the
recent introduction of a modified Clojure Cheat Sheet (thanks to Andy
Fingerhut &amp;amp; contributors) as Grimoire&amp;#39;s primary index.&lt;/p&gt;

&lt;p&gt;Something else I would like to explore with Grimoire is automated
analysis and link generation between examples. In ClojureDocs, (and I
admit Grimoire as it stands) examples were static text analyzed for
syntax before display to users. As part of my work on Oxcart, I had an
idea for how to build a line information table from symbols to binding
locations and I&amp;#39;d like to explore providing examples with inline links
to the documentation of other functions used.&lt;/p&gt;

&lt;p&gt;Finally I&amp;#39;d like to go beyond Clojure&amp;#39;s documentation. ClojureDocs and
Grimoire at present only present the official documentation of Clojure
functions. However some of Clojure&amp;#39;s behavior such as the type
hierarchy is not always obvious.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;lt; amalloy&amp;gt; ~colls&lt;/p&gt;

&lt;p&gt;&amp;lt; Clojurebot&amp;gt; colls is &lt;a href=&quot;http://www.brainonfire.net/files/seqs-and-colls/main.html&quot;&gt;http://www.brainonfire.net/files/seqs-and-colls/main.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Frankly the choice of the word Grimoire is a nod in this
direction... a joke as it were on
&lt;a href=&quot;http://dota2.gamepedia.com/Invoker#Bio&quot;&gt;Invoker&amp;#39;s&lt;/a&gt; fluff and the
Archmagus like mastery which I see routinely exhibited on Freenode&amp;#39;s
Clojure channel of the many ins and outs of the language while I
struggle with basic stuff like &amp;quot;Why don&amp;#39;t we have
&lt;code&gt;Clojure.core/atom?&lt;/code&gt;&amp;quot; and &amp;quot;why is a Vector not &lt;code&gt;seq?&lt;/code&gt; when it is
Sequable and routinely used as such?&amp;quot;. &amp;quot;Why don&amp;#39;t we have
&lt;code&gt;Clojure.core/sequable?&lt;/code&gt;&amp;quot;?&lt;/p&gt;

&lt;p&gt;Clojure&amp;#39;s core documentation doesn&amp;#39;t feature type signatures, even
types against Clojure&amp;#39;s data interfaces. I personally find that I
think to a large degree in terms of the types and structure of what
I&amp;#39;m manipulating I find this burdensome. Many docstrings are simply
wanting if even present. I think these are usability defects and would
like to explore augmenting the &amp;quot;official&amp;quot; Clojure
documentation. &lt;a href=&quot;https://github.com/jafingerhut/thalia&quot;&gt;Andy Fingerhut&amp;#39;s thalia&lt;/a&gt;
is another effort in this direction and one which I hope to explore
integrating into Grimoire as non-invasively as possible.&lt;/p&gt;

&lt;h2&gt;Status&lt;/h2&gt;

&lt;p&gt;Much of what I have talked about here is work that needs to be
done. The first version of Grimoire that I announced originally on the
Clojure mailing list was a trivial hierarchical directory structure
aimed at users who sorta kinda knew what they were looking for and
where to find it because that&amp;#39;s all I personally need out of a
documentation tool for the most part after two years of non-stop
Clojure. Since then I&amp;#39;ve been delighted to welcome and incorporate
criticism and changes from those who have found Grimoire similarly of
day to day use, however I think it&amp;#39;s important to note that Grimoire
is fundamentally user land tooling as is Leiningen and as is Thalia.&lt;/p&gt;

&lt;p&gt;As such, I don&amp;#39;t expect that Grimoire will ever have any official
truck with Rich&amp;#39;s sanctioned documentation. This and the hope that we
may one day get better official docs mean that I don&amp;#39;t really foresee
migrating Grimoire off of my personal hosting to a more &amp;quot;Clojure-ey&amp;quot;
domain. Doing so would lend Grimoire an undue level of
&amp;quot;official-ness&amp;quot;, forget the fact that I&amp;#39;m now rather attached to the
name and that my browser goes to the right page with four keystrokes.&lt;/p&gt;

&lt;h2&gt;Future&lt;/h2&gt;

&lt;p&gt;As far as I am concerned, Grimoire is in a good place. It&amp;#39;s under my
control, I have shall we say a side project as I&amp;#39;m chugging along on
&amp;quot;day job&amp;quot; for GSoC and it seems judging by Google Analytics that some
300 other Clojureists have at least bothered to stop by and some 70
have found Grimoire useful enough to come back for more all in the
three days that I&amp;#39;ve had analytics on. There is still basic UI work to
be done, which isn&amp;#39;t surprising because I claim no skill or taste as a
graphic designer, and there&amp;#39;s a lot of stuff I&amp;#39;d like to do in terms
of improving upon the existing documentation.&lt;/p&gt;

&lt;p&gt;Frankly I think it&amp;#39;s pretty crazy that I put about 20 hours of effort
into something, threw it up on the internet and suddenly for the first
time in my life I have honest to god &lt;em&gt;users&lt;/em&gt;. From 38 countries. I
should try this &amp;quot;front end&amp;quot; stuff more often.&lt;/p&gt;

&lt;p&gt;So here&amp;#39;s hoping that Grimoire continues to grow, that other people
find it useful, and that I manage to accomplish at least some of the
above. While working on Oxcart. And summer classes. Yeah.....&lt;/p&gt;

&lt;p&gt;^D&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Oxcart and Clojure</title>
    <link href="http://arrdem.com/2014/06/26/oxcart_and_clojure/"/>
    <updated>2014-06-26T20:18:45+00:00</updated>
    <id>http://arrdem.com/2014/06/26/oxcart_and_clojure</id>
    <content type="html">&lt;p&gt;Well, it&amp;#39;s a month into
&lt;a href=&quot;https://www.google-melange.com/&quot;&gt;Google Summer of Code&lt;/a&gt;, and I still
haven&amp;#39;t actually written anything about
&lt;a href=&quot;https://www.google-melange.com/gsoc/project/details/google/gsoc2014/arrdem/5676830073815040&quot;&gt;my project&lt;/a&gt;,
better known as &lt;a href=&quot;https://github.com/arrdem/oxcart&quot;&gt;Oxcart&lt;/a&gt; beyond what
little per function documentation I have written for Oxcart and the
&lt;a href=&quot;http://us4.campaign-archive1.com/?u=a33b5228d1b5bf2e0c68a83f4&amp;amp;id=6ee36652d6&quot;&gt;interview I did with Eric N.&lt;/a&gt;. So
it&amp;#39;s time to fix that.&lt;/p&gt;

&lt;h2&gt;Motivation&lt;/h2&gt;

&lt;p&gt;Clojure isn&amp;#39;t &amp;quot;fast&amp;quot;, it&amp;#39;s simply &amp;quot;fast enough&amp;quot;. Rich, while really
smart guy with awesome ideas isn&amp;#39;t a compilers research team and
didn&amp;#39;t design Clojure with a laundry list of tricks that he wanted to
be able to play in the compiler in mind. Instead in the beginning,
Rich designed a language that he wanted to use to do work, built a
naive compiler for it, confirmed that JVM JITs could run the resulting
code sufficiently fast, and got on with actually building things.&lt;/p&gt;

&lt;p&gt;The consequent of Rich&amp;#39;s priorities is that Clojure code is in fact
fast enough to do just about whatever you want, but it could be
faster. Clojure is often criticized for its slow startup time and its
large memory footprint. Most of this footprint is not so much a
consequent of fundamental limitations of Clojure as a language (some
of it is but that&amp;#39;s for another time) as it is a consequent of how the
existing Clojure compiler runtime pair operate together.&lt;/p&gt;

&lt;p&gt;So Clojure wasn&amp;#39;t designed to have a sophisticated compiler, it
doesn&amp;#39;t have such a compiler, and for some applications Clojure is
slow compared to other equivalent languages as a result of not having
these things. So for GSoC I proposed to build a prototype compiler
which would attempt to build Clojure binaries tuned for performance
and I got accepted.&lt;/p&gt;

&lt;h2&gt;Validating complaints&lt;/h2&gt;

&lt;p&gt;Okay, so I&amp;#39;ve made grand claims about the performance of Clojure, that
it could be faster and soforth. What exactly do I find so distasteful
in the language implementation?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vars&lt;/strong&gt; are the first and primary whipping boy. &lt;code&gt;Vars&lt;/code&gt;,
&lt;a href=&quot;https://github.com/Clojure/Clojure/blob/master/src/jvm/Clojure/lang/Var.java&quot;&gt;defined over here&lt;/a&gt;,
are data structures which Clojure uses to represent bindings between
symbols and values. These bindings, even when static at compile time,
are interred at runtime in a thread shared global bindings table, and
then thread local bindings tables contain &amp;quot;local&amp;quot; bindings which take
precedence over global bindings. This is why
&lt;code&gt;(Clojure.core/alter-var-root!)&lt;/code&gt; and the other var manipulation
functions in the Clojure standard library have the &lt;code&gt;-!&lt;/code&gt; postfix used
to annotate transactional memory mutation, because only a transaction
can modify the root bindings of a thread shared and thread safe &lt;code&gt;Var&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now &lt;code&gt;Var&lt;/code&gt;s are awesome because they are thread shared. This means that
if you drop a REPL in a live program you can start re-defining &lt;code&gt;Var&lt;/code&gt;s
willy nilly and your program will &amp;quot;magically&amp;quot; update. Why does this
work? Because Clojure programs never hold a reference to a &amp;quot;function&amp;quot;,
instead they hold a thread synchronized var which names a function and
get the latest function named by the var every time they have to call
that function.&lt;/p&gt;

&lt;p&gt;This is great because it enables the REPL driven development and
debugging pattern upon which many people rely, however for the
overwhelming majority of applications the final production form of a
program will never redefine a &lt;code&gt;Var&lt;/code&gt;. This means that consequently the
nontrivial overhead of performing thread synchronization, fetching the
function named by a var, checking that the fn is in fact
&lt;code&gt;Clojure.lang.IFn&lt;/code&gt; and then calling the function is wasted overhead
that the reference Clojure implementation incurs every single function
call. The worst part about this is that &lt;code&gt;Var&lt;/code&gt;
&lt;a href=&quot;https://github.com/Clojure/Clojure/blob/master/src/jvm/Clojure/lang/Var.java#L22&quot;&gt;has a volatile root&lt;/a&gt;
which poisons the JVM&amp;#39;s HotSpot JIT by providing a break point at
function boundaries which the JIT can&amp;#39;t inline or analyze across.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;IFn&lt;/strong&gt; is another messy part of Clojure programs. The JVM does not
have semantics for representing a pointer to a &amp;quot;function&amp;quot;. The result
is that Clojure doesn&amp;#39;t really have &amp;quot;functions&amp;quot; when compiled to JVM
bytecode, instead Clojure has classes providing &lt;code&gt;.invoke()&lt;/code&gt; methods
and instances of those classes, or more often &lt;code&gt;Var&lt;/code&gt;s naming &lt;code&gt;IFn&lt;/code&gt;s may
be passed around as values. This isn&amp;#39;t a bad thing for the most part,
except that &lt;code&gt;IFn&lt;/code&gt;s use &lt;code&gt;Var&lt;/code&gt;s to implement their &lt;code&gt;.invoke()&lt;/code&gt; methods
and vars are slow.&lt;/p&gt;

&lt;p&gt;The real reason that this can be an issue is why do we need an &lt;code&gt;IFn&lt;/code&gt;
with multiple invoke methods? Because in Clojure functions can have
multiple arities, and the single instance of an &lt;code&gt;IFn&lt;/code&gt; could be invoked
multiple ways where a JVM Method cannot be.&lt;/p&gt;

&lt;p&gt;The real issue with &lt;code&gt;IFn&lt;/code&gt;s is that they exist at all. Every single
class referenced by a JVM program must be read from disk, verified,
loaded and compiled. This means that there is a load time cost to each
individual class in a program. Clojure exacerbates this cost by not
only generating a lot of small classes to implement &lt;code&gt;IFn&lt;/code&gt;s. When a
namespace is &lt;code&gt;require&lt;/code&gt;d or otherwise &lt;code&gt;load&lt;/code&gt;ed by a compiled Clojure
program, the Clojure runtime loads &lt;code&gt;foo__init.class&lt;/code&gt;, which creates
instances of every &lt;code&gt;IFn&lt;/code&gt; used to back a &lt;code&gt;Var&lt;/code&gt; in the namespace &lt;code&gt;foo&lt;/code&gt;
and installs those definitions in the global var name table. Note that
this loading is single threaded, so all synchronization at load time
is wasteful. Also note that if there are top level forms like
&lt;code&gt;(println &amp;quot;I got loaded!&amp;quot;)&lt;/code&gt; in a Clojure program those are evaluated
when the namespace containing the form is loaded.&lt;/p&gt;

&lt;h2&gt;The sales pitch&lt;/h2&gt;

&lt;p&gt;So what&amp;#39;s the short version here? Clojure has &lt;code&gt;Var&lt;/code&gt;s in order to
enable a dynamic rebinding model of programming which deployed
applications do not typically need. Because applications do not tend
to use dynamic binding for application code, we can discard &lt;code&gt;Var&lt;/code&gt;s and
directly use the &lt;code&gt;IFn&lt;/code&gt; classes to which the &lt;code&gt;Var&lt;/code&gt;s refer. This could
be a significant win just because it removes that &lt;code&gt;volatile&lt;/code&gt; root on
&lt;code&gt;Var&lt;/code&gt;s that poisons the JIT.&lt;/p&gt;

&lt;p&gt;This opens up more opportunities for playing tricks in the compiler,
because we don&amp;#39;t really need &lt;code&gt;IFn&lt;/code&gt; objects for the most part. Remember
that &lt;code&gt;IFn&lt;/code&gt; objects are only needed because methods aren&amp;#39;t first class
on the JVM and to support dynamic redefinitions we need a first class
value for &lt;code&gt;Var&lt;/code&gt;s to point to. If all definitions are static, then we
don&amp;#39;t need &lt;code&gt;Var&lt;/code&gt;s, so we can find the fully qualified class and method
that a given function invocation points to, freeing a Clojure compiler
to do static method invocation. This should be a performance win as it
allows the JVM JIT to escape type checking of the object on which the
method is invoked and it allows the JIT to inline in the targeted
method among other tricks.&lt;/p&gt;

&lt;p&gt;If we can throw away &lt;code&gt;IFn&lt;/code&gt;s by implementing functions as say static
methods on a namespace &amp;quot;class&amp;quot; rather than having seperate classes for
each function, then we cut down on program size in terms of classes
which should somewhat reduce the memory footprint of Clojure programs
on disk and in memory in addition to reducing load time.&lt;/p&gt;

&lt;h2&gt;Oxcart&lt;/h2&gt;

&lt;p&gt;So what is Oxcart? Oxcart is a compiler for a subset of Clojure which
seeks to implement exactly the performance hat tricks specified above
and a few more. For the most part these are simple analysis operations
with well defined limitations. In fact, most of what&amp;#39;s required to use
Oxcart to compile Clojure code is already built and working in that
Oxcart can currently rewrite Clojure programs to aid and execute the
above transformations.&lt;/p&gt;

&lt;p&gt;Oxcart is also a huge exercise in the infrastructure around the
&lt;code&gt;Clojure.tools.analyzer&lt;/code&gt; contrib library, as Oxcart is the first full
up compiler to use &lt;code&gt;tools.analyzer&lt;/code&gt;, &lt;code&gt;tools.analyzer.jvm&lt;/code&gt; and
&lt;code&gt;tools.emitter.jvm&lt;/code&gt; as more than the direct compilation pipeline which
&lt;code&gt;tools.emitter.jvm&lt;/code&gt; implements. This means that Oxcart has interesting
representational issues in how passes and analysis data are handled
and shared between passes, let alone how the data structure describing
the entire program is built and the performance limitations of various
possible representations thereof.&lt;/p&gt;

&lt;p&gt;So what&amp;#39;s Oxcart good for? right now: nothing. Oxcart doesn&amp;#39;t have an
AOT file emitter yet, and relies on &lt;code&gt;tools.emitter.jvm&lt;/code&gt; for &lt;code&gt;eval&lt;/code&gt; and
as such is no faster for &lt;code&gt;eval&lt;/code&gt;ing code in a live JVM than Clojure
is. At present I&amp;#39;m working on building an AOT emitter which will
enable me to start doing code generation and profiling Oxcart against
Clojure. I hope to post an initial emitter and a trivial benchmark
comparing a pair of mutually recursive math functions between Clojure
and Oxcart.&lt;/p&gt;

&lt;h2&gt;Before you go&lt;/h2&gt;

&lt;p&gt;I&amp;#39;ve know I&amp;#39;ve said this entire time that Oxcart is a Clojure
compiler. That&amp;#39;s a misnomer. Oxcart doesn&amp;#39;t compile Clojure and never
will. Clojure has stuff like &lt;code&gt;eval&lt;/code&gt;, &lt;code&gt;eval-string&lt;/code&gt;, &lt;code&gt;resolve&lt;/code&gt;,
&lt;code&gt;load-string&lt;/code&gt;, &lt;code&gt;load&lt;/code&gt; and all the &lt;code&gt;bindings&lt;/code&gt; stuff that allow Clojure
programmers to reach around the compiler&amp;#39;s back and change bindings
and definitions at runtime. These structures are not and never will be
supported by Oxcart because supporting them would require disabling
optimizations. Oxcart also doesn&amp;#39;t support non-def forms at the top
level. Oxcart programs are considered to be a set of defs and an entry
point. Oxcart also assumes that definitions are single. Redefining a
var is entirely unsupported, abet not yet a source of warnings.&lt;/p&gt;

&lt;p&gt;Some of these differences are sufficiently extreme that I&amp;#39;m honestly
on the fence about whether Oxcart is really Clojure or some yet
undefined &amp;quot;Oxlang&amp;quot; more in the style of Shen, but for now I&amp;#39;ll stick
to building a prototype &lt;code&gt;:D&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;^D&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Dogecoin block pricing experiment</title>
    <link href="http://arrdem.com/2014/05/22/dogecoin-pricing/"/>
    <updated>2014-05-22T00:00:00+00:00</updated>
    <id>http://arrdem.com/2014/05/22/dogecoin-pricing</id>
    <content type="html">&lt;p&gt;As I&amp;#39;ve explained repeatedly before, the block reward is the mechanism
which we as a community use to buy the hashing power which keeps our
network relatively independant and secure as an honest processor of
users transactions. As we cannot pay miners in fiat somehow, we pay
them in Doge in the form of block rewards and transaction fees. This
means that we as a community are putting a price on the hashing power
used to secure our network. I thought it would be interesting to try
and figure out what that number is.&lt;/p&gt;

&lt;p&gt;The reason it&amp;#39;s interesting is the Dogecoin block reward schedule. To
encourage mining and manage the value of the coin, Doge operates on a
diminishing rewards block schedule that encourages mining and buying
into Doge with hashing power by rewarding early miners more than late
miners. However this works only to a point. Mining Doge has to be ROI
positive or nobody would bother with it. Mining Doge also has to be
ROI competitive with other altcoins or nobody would bother with
it. This first requirement I address in this post, and the second
requirement I&amp;#39;ll address some other time.&lt;/p&gt;

&lt;h1&gt;Experiment &lt;/h1&gt;

&lt;p&gt;This is current (ish) pricing and power consumption information for a
variety of different mining hardware. The important number here is the
final column, KH/w. As the point of this exercise is to put a Doge
price on every block, the power consumption of every KH pointed at
mining Doge counts.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt; Vendor      Miner              KH/s       Watt   Cost           KH/w  
-----------+------------------+---------+-------+-----------+---------
 KnCMiner    Mini Titan         150,000     400   $5,495.00    375.00  
 KnCMiner    Titan              300,000     800   $9,995.00    375.00  
 Gridseed    ASIC Blade Miner   5,200        70   $1,049.95     74.29  
 Gridseed    ASIC 5-Chip        350           7   $79.95        50.00  
 DualMiner   ASIC USB 2         70            2   $81.99        46.67  
 GAWMiner    War Machine        54,000    1,280   $5,899.95     42.19  
 GAWMiner    Black Widow        13,000      320   $1,799.95     40.63  
 GAWMiner    Fury               1,000        30   $159.95       33.33  
 DualMiner   ASIC USB 1         70            3   $98.00        28.00  
 Radeon      R9 290X            850         295   $579.99        2.88  
 NVIDIA      GTX 770            220         230   $329.99        0.96  
 NVIDIA      GTX 560 Ti         150         170   $120.00        0.88  
-----------+------------------+---------+-------+-----------+---------
 Average                                                      89.1525  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At present Doge is near 1K Doge = 0.435 USD, or measured in doge/usd
2298.85.&lt;/p&gt;

&lt;h2&gt;Measuring the price per block &lt;/h2&gt;

&lt;p&gt;power costs 0.1220 USD/KWhr at US national average prices, so lets run
some numbers.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-clojure&quot; data-lang=&quot;clojure&quot;&gt;&lt;span class=&quot;lineno&quot;&gt; 1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;ns &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;doge&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 2&lt;/span&gt;   &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:require&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;meajure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 3&lt;/span&gt;             &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;Clojure.algo.generic.arithmetic&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 4&lt;/span&gt;              &lt;span class=&quot;ss&quot;&gt;:refer&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;+ - / &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]))&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 5&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt; 6&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;; constants and definitions&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 7&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;;------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 8&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt; 9&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;kw-per-w&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;10&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;meajure/unit&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;/1000&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:kwatt&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:watt&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;; definition&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;11&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;12&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;hr-per-block&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;13&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;meajure/unit&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;/60&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:hour&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:block&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;; definition&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;14&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;15&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;16&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;; simulation variables&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;17&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;;------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;18&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;19&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;kh-per-watt&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;20&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;meajure/unit&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;375&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;21&lt;/span&gt;                  &lt;span class=&quot;ss&quot;&gt;:khash&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:watt&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;-1&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:second&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;; variable&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;22&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;23&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;usd-per-watt-block&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;24&lt;/span&gt;   &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;* &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;meajure/unit&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.122&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:usd&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:kwatt&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;-1&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:hour&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;; variable&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;25&lt;/span&gt;      &lt;span class=&quot;nv&quot;&gt;kw-per-w&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;26&lt;/span&gt;      &lt;span class=&quot;nv&quot;&gt;hr-per-block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;27&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;28&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;doge-per-usd&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;29&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;meajure/unit&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2298.85&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:doge&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:usd&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;; variable&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;30&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;31&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;; computation&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;33&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;;------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;34&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;35&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;price-per-watt-block&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;36&lt;/span&gt;   &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;/ &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;doge-per-usd&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;kh-per-watt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;37&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;38&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;print &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;DOGE per watt-block |&amp;quot;&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;39&lt;/span&gt;        &lt;span class=&quot;nv&quot;&gt;price-per-watt-block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;40&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;41&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;price-per-kh&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;42&lt;/span&gt;   &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;* &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;usd-per-watt-block&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;43&lt;/span&gt;      &lt;span class=&quot;nv&quot;&gt;price-per-watt-block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;44&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;45&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;print &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;DOGE per KH         |&amp;quot;&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;46&lt;/span&gt;        &lt;span class=&quot;nv&quot;&gt;price-per-kh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;47&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;48&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;; break even block reward&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;49&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;print &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;DOGE for hashrate   |&amp;quot;&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;50&lt;/span&gt;   &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;* &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;price-per-kh&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;51&lt;/span&gt;      &lt;span class=&quot;o&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;meajure/unit&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:mega&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:khash&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:second&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;DOGE per watt-block | #meajure/unit [25.785592103418296,
                                     :second, :watt, :khash -1, :usd -1, :doge]

DOGE per KH         | #meajure/unit [5.243070394361721E-5,
                                     :doge, :khash -1, :second, :block -1]

DOGE for hashrate   | #meajure/unit [2621.5351971808605,
                                     :doge, :block -1]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Analysis &lt;/h2&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt; Miner              KH/s       Watt      KH/w   Min. block reward  
------------------+---------+-------+---------+-------------------
 Mini Titan         150,000     400    375.00       623.243777777  
 Titan              300,000     800    375.00       623.243777777  
 ASIC Blade Miner   5,200        70     74.29      3158.329954954  
 ASIC 5-Chip        350           7     50.00      4674.328333333  
 ASIC USB 2         70            2     46.67      5007.851224910  
 War Machine        54,000    1,280     42.19      5539.616417790  
 Black Widow        13,000      320     40.63      5752.311510378  
 Fury               1,000        30     33.33      7012.193719371  
 ASIC USB 1         70            3     28.00      8347.014880952  
 R9 290X            850         295      2.88     81151.533564810  
 GTX 770            220         230      0.96    243454.600694440  
 GTX 560 Ti         150         170      0.88    265586.837121212  
------------------+---------+-------+---------+-------------------
 Average                              89.1525           28935.106  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In order to recieve any return on invested compute power, a miner must
find blocks. If the difficulty is too high, then it isn&amp;#39;t feasable to
find blocks and mining is ROI negative. This is why Wafflepool and
other multipools switch coins: they mine for as long as it&amp;#39;s ROI
positive to do so and once the difficulty adjusts rendering it no
longer ROI positive to do so they bail out and head for greener
pastures.&lt;/p&gt;

&lt;p&gt;So, Dogecoin has a minimum block reward of 10,000 Doge, but we won&amp;#39;t
see that until January. At present we&amp;#39;re at a 125,000 Doge block
reward. Looking at this table, that indicates that Nvidia GPUs are now
absolutely ROI negative. I won&amp;#39;t be mining any more for this reason.&lt;/p&gt;

&lt;p&gt;You don&amp;#39;t just get coins for mining tho, you get coins for finding
blocks. Assuming that the Scrypt proof of work function is inviolate,
what fraction of the hashrate do each of these miners need to
represent at the current block reward to break even?&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt; Miner                KH/s   Min. block reward   Hashrt. Frac.   Hashrt. (KH/s)  
------------------+--------+-------------------+---------------+----------------
 Mini Titan         150000       623.243777777    4.9859502e-3       30084.5360  
 Titan              300000       623.243777777    4.9859502e-3       60169.0730  
 ASIC Blade Miner     5200      3158.329954954     0.025266640      205804.9700  
 ASIC 5-Chip           350      4674.328333333     0.037394627        9359.6334  
 ASIC USB 2             70      5007.851224910     0.040062810        1747.2564  
 War Machine         54000      5539.616417790     0.044316931     1218495.9000  
 Black Widow         13000      5752.311510378     0.046018492      282495.1300  
 Fury                 1000      7012.193719371     0.056097550       17826.0900  
 ASIC USB 1             70      8347.014880952     0.066776119        1048.2790  
 R9 290X               850     81151.533564810      0.64921227        1309.2790  
 GTX 770               220    243454.600694440       1.9476368         112.9574  
 GTX 560 Ti            150    265586.837121212       2.1246947          70.5983  
------------------+--------+-------------------+---------------+----------------
 Average                             28935.106                                   
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And what&amp;#39;s our current hashrate? 48GH/s, or 48000000 KH/s.&lt;/p&gt;

&lt;h1&gt;Conclusion &lt;/h1&gt;

&lt;p&gt;I think the writing&amp;#39;s on the wall here. If everyone on the Dogecoin
network was running the War Machine miner, our hashrate is a factor of
48 times higher than would be break even. What does this mean for our
future? As block rewards fall, either the price of Doge will rise due
to adoption as a unit of trade which will drive down the minimum block
reward numbers, decreasing the hashrate fractions and increasing the
global break even network hashrate.&lt;/p&gt;

&lt;p&gt;What do I think this means? Well after fooling around with this math
and the price of Doge variable the computed maximum network hashrate
goes over 45GH only when the price of Doge increases to about 1523
Doge/USD, or at current market prices about 130 Satoshi BTC. If the
price of Doge doesn&amp;#39;t rise back to the 130 mark, I expect that we will
see our hashrate slowly but steadily fall until it reaches a point
where miners perceive that they are breaking even, being very likely
under 1GH/s according to these numbers.&lt;/p&gt;

&lt;p&gt;If we want Dogecoin to grow into something more than a tip currency I
think that we missed the boat when we turned down merged mining with
LTC. We have to secure our hashrate for the long haul that it&amp;#39;ll take
us to build real market valuation through acceptance as a unit of
trade. Merged mining with LTC would have achieved that goal. So what
do I think the outlook is? I think we&amp;#39;re gonna run out of fuel in
earth orbit rather than get to the moon.&lt;/p&gt;

&lt;p&gt;^d&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Dogecoin part II</title>
    <link href="http://arrdem.com/2014/04/25/dogecoin-ii/"/>
    <updated>2014-04-25T00:00:00+00:00</updated>
    <id>http://arrdem.com/2014/04/25/dogecoin-ii</id>
    <content type="html">&lt;p&gt;This post originally appeared on reddit.com/r/dogecoin, and was added
to my official tech blog for posterity. or something.&lt;/p&gt;

&lt;p&gt;&lt;hr /&gt;&lt;/p&gt;

&lt;p&gt;Hey shibes, /u/arrdem back again with more science and musing on the
future of our beloved
coin. &lt;a href=&quot;/2014/04/24/dogecoin-i.html&quot;&gt;last time&lt;/a&gt;,
I talked in generalities about the impact of ASICs on the price of
DOGE and their impact on the dogeconomy in general. Today I&amp;#39;d like to
go into some more depth on ASICs, proof of work, proof of stake and
the future of DOGE as we look ahead and think about long term
valuation. I got some amazing feedback and insightful comments, which
I thought warranted a follow up article. So here we go!&lt;/p&gt;

&lt;h2&gt;From last time&lt;/h2&gt;

&lt;p&gt;Last time when I wrote, I mentioned the
&lt;a href=&quot;http://en.wikipedia.org/wiki/Dogecoin#Block_schedule&quot;&gt;dogecoin block schedule&lt;/a&gt;
and pointed out that the rate at which we are issuing DOGE is such
that most of the second and third generations ASICs which are being
manufactured targeting Scrypt coins will never see &amp;quot;high&amp;quot; returns per
block simply due to the impending halvening(s).&lt;/p&gt;

&lt;h2&gt;On valuation&lt;/h2&gt;

&lt;p&gt;An open question for cryptocurrencies is where do we derive valuation
from? What justifies Bitcoin&amp;#39;s standing $500 prices? The Bitcoin crowd
looks to
&lt;a href=&quot;http://en.wikipedia.org/wiki/Austrian_School&quot;&gt;Austrian economics&lt;/a&gt; and
the fact that Bitcoin is asymptotically finite resource as the sources
of their valuation.&lt;/p&gt;

&lt;p&gt;Now I&amp;#39;ll confess that I&amp;#39;m a skeptic of both Bitcoin and Dogecoin as
they stand today. As a computer scientist, Bitcoin is fascinating
because the blockchain and proof of work which Satoshi introduced is a
viable solution to the
&lt;a href=&quot;http://en.wikipedia.org/wiki/Two_Generals%27_Problem&quot;&gt;Two Generals Problem&lt;/a&gt;.
&lt;a href=&quot;http://expectedpayoff.com/blog/2013/03/22/bitcoin-and-the-byzantine-generals-problem/&quot;&gt;This article&lt;/a&gt;
gives a good summary of the significance of this discovery. So what
does this mean? It means that the Bitcoin protocol which underlies
Bitcoin, Dogecoin and all the others represents a fundamental advance
in the design of distributed systems. That this advance is used to
implement a distributed, verifiable &amp;quot;value&amp;quot; (coin) store or
cryptocurrency is just the first of many problems which I expect to
see revolutionized by cryptocurrency derived
software. &lt;a href=&quot;https://www.namecoin.org/&quot;&gt;Namecoin&lt;/a&gt; is a fascinating
example of the wider value of the blockchain technology.&lt;/p&gt;

&lt;p&gt;So the blockchain itself as a technology is interesting. What does
this actually mean for cryptocurrencies? It means that
cryptocurrencies are potentially the ultimate value refuge. If we
assume that there is no heartbleed scale flaw in Bitcoin derived
networks which allows for private keys to be stolen and we assume that
coin holding entities are capable of securely storing their private
keys it is impossible for computational power less than 51% of the
coin network to steal coins. However at the magic 51% tipping point a
malicious majority can start writing fake transactions into the
blockchain and generally ruin everything.&lt;/p&gt;

&lt;p&gt;Another reason to hope for the value of the coins is that they provide
more secure transaction capabilities than credit cards do. As we were
reminded with the Target hack, in order to process one credit card
transactions, companies who accept credit cards need enough
information to process an arbitrary number of arbitrary sized
trasactions. The cryptocurrencies do much better at this, as it is
impossible for a payment recipient to &amp;quot;charge&amp;quot; repeatedly. Thanks to
multiple signature transactions we&amp;#39;ve even solved the escrow problem,
although escrow providing payment processors for coin based purchases
still need to mature.&lt;/p&gt;

&lt;p&gt;The last significant source of value I see is the low transaction
costs. I happen to be DLLAzkmxUypWnF6TvdmCVwaDrro1isqiWh, and knowing
that address anyone with the dogecoin client and spare doge can throw
some my way at a marginal cost per transaction of $0.00 no matter how
large the transaction compared to the 5% and 10% fees chared by other
online payment processors such as Square and the existing credit card
companies. This means that as we see with DOGE the community can at
near zero marginal cost donate to causes anywhere in the world,
further increasing the potential impact and reach of previously
centralized crowdfunding efforts.&lt;/p&gt;

&lt;h2&gt;In the shadow of the moon&lt;/h2&gt;

&lt;p&gt;Now there&amp;#39;s a dark underside to the blockchain techology: the proof of
work function which Satoshi presented. The blockchain only functions
because any entity who wishes to intern records in the blockchain must
perform a significant and fundimentally worthless computation in order
to prove their honesty. Bitcoin uses the sha256 hash function, we use
the Scrypt hash function. Some coins, such as
&lt;a href=&quot;http://primecoin.io/&quot;&gt;Primecoin&lt;/a&gt; have taken an interesting approach
to the fundamental worthlessness of the sha256sum and Scrypt
computations by trying to use scientific computing problems as proofs
of work.&lt;/p&gt;

&lt;p&gt;Primecoin is in an interesting place, because verifying primality of a
number or number sequence is comparatively easy. Other proposals
especially for protein folding based coins, are ultimately doomed
because the difficulty of verifying a solution to some folding problem
is equivalent to solving it in the first place. This means that
verifying blocks and the transactions encoded therein is exceedingly
slow and hard, which decreases the micropayment utility of the coin.&lt;/p&gt;

&lt;p&gt;However, mining is ultimately a network mechanism for securely
processing transactions. It has a cost and no value. The per block
reward mechanism first used in Bitcoin is a mechanism for purchasing
the mining power required to securely process transactions by
inflating the coin and expecting that miners will speculate against
the future value of the coin to justify their mining costs. Note that
this is fundimentally circular. The initial miners are highly rewarded
for supporting the network at its most vulneralbe, with miners who
join only after the coin has achieved some modicum of stability being
less strongly rewarded.&lt;/p&gt;

&lt;p&gt;This fundamental &amp;quot;get in early&amp;quot; reward for mining is why we see
altcoins or shitcoins pop up and die right off either having been
maliciously 51%&amp;#39;d as may have been the case with Arouracoin. What we
seem to be seeing with the price of DOGE is that the BTC/MH/Hr return
of mining DOGE isn&amp;#39;t high enough to attract new miners. Using data
from http://liteshack.com/ We&amp;#39;re sitting at about ~30% of the script
hashing power not pointed at Litecoin, and
&lt;a href=&quot;http://liteshack.com/?coin=doge&quot;&gt;~19%&lt;/a&gt; of the total Scrypt hashing
power. This means that, as /u/listlesscraig
&lt;a href=&quot;http://www.reddit.com/r/dogecoin/comments/23lzt3/serious_of_prices_asics_and_x11/cgyeqfu&quot;&gt;pointed out&lt;/a&gt;
on my original post, we could very easily as a coin be crushed by a
malicious 51% attack should someone on the Litecoin or Wafflepool side
of things decide that they really really had it in for us.&lt;/p&gt;

&lt;p&gt;So what does this mean? It means that in order to be secure as a coin,
our hashrate needs to increase significantly. We either need to pay
more for mining, or we need to decrease our dependance on mining (more
on that later).&lt;/p&gt;

&lt;p&gt;With the block 200001 halvening about 48hrs out, we&amp;#39;ll see the per
block reward drop to 125000Ɖ.  At current market prices (115 satoshi
as of this writing) that pegs our per block reward at
0.14375BTC/block, or $71.88/block. So at our current network hashrate
of a 62.449GH average, which means we&amp;#39;re effectively paying a
0.0000000023BTC/Kh/min price for our hashing power. In comparison,
mining Litecoin pays out at 0.00000000304BTC/Kh/min by my math. As our
return is lower than LTCs and new hyped alts (shitcoins) are even
higher, I think it&amp;#39;s clear that we are unlikely to see Wafflepool or
other significant sources of hashing power come back without a price
jump into the 200s at least.&lt;/p&gt;

&lt;p&gt;Now I can sit here and run numbers, but clearly DOGE while featuring
an awesome community is not at present a high ROI coin which means we
are not accumulating the hashing power needed to secure ourselves
against potential machinations of present whale miners ignoring the
potential threat of future ASICs.&lt;/p&gt;

&lt;p&gt;Now, some people including /u/GoodShibe have been known to call for
community mining effort. However he&amp;#39;s the good shibe and I&amp;#39;m the math
shibe. On average, the current generation of graphics cards can do
about ~360KH/s, so if we assume that every subscriber on this
subreddit has a &amp;quot;real&amp;quot; graphics card, that gives us a combined
hashrate of 28,395,000KH/s, which is 44.75% of the current network
hashrate. Assuming that error due to shibes rocking hundred GPU rigs
and shibes who CPU mine average out, I think this number makes it
clear that as a community we can&amp;#39;t raise the hashing power that we
&amp;quot;need&amp;quot; to armor ourselves against the potential machinations of whales
and pools.&lt;/p&gt;

&lt;h2&gt;Adjusting launch window&lt;/h2&gt;

&lt;p&gt;As we can&amp;#39;t raise the raw hashing power that&amp;#39;s required to maintain
our beloved coin and our market valuation continues to fall, it seems
to me that there are two plans of action. The first is to try to raise
awareness of Dogecoin through publicity stunts like Doge4water and
Dogecar. I hope that these both prove to have been positive forces in
the long run. Ultimately however, Dogecoin is only as useful as
Bitcoin or Litecoin is if we can only tip each other, donate and
hoard. Payment processor adoption and getting businesses to start
accepting cryptocurrencies are the real way to secure and increase our
long term valuation.&lt;/p&gt;

&lt;p&gt;In the short term we need to secure ourselves as well so we can get to
the long term. Clearly as a community we don&amp;#39;t have the kind of
hashing power we need to do it ourselves with the &amp;quot;classical&amp;quot; proof of
work structure, so I suggest that we look elsewhere, especially to
&lt;a href=&quot;https://en.bitcoin.it/wiki/Proof_of_Stake&quot;&gt;proof of stake&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Proof of stake is an alternative transaction verification scheme under
which &amp;quot;miners&amp;quot; mine not on the basis of how much computational power
they have, but on the basis of how many coins they control. This means
that the mining operation can be much less computationally expensive,
and that a 51% attack is impossible except from an entity with a
significant stake in the coin who would stand to loose much in the
fallout from DOGE&amp;#39;s destruction.&lt;/p&gt;

&lt;p&gt;By adopting proof of stake, we would make it much more difficult for a
wild whale to crush us whether out of malice or by accident. However
most importantly we increase our own perceived security and stability
which clears the way for more adoption down the road.&lt;/p&gt;

&lt;h2&gt;Afterward&lt;/h2&gt;

&lt;p&gt;I look forwards to seeing what you think of this piece. Frankly I ran
the numbers as I wrote it, and I must say I&amp;#39;m saddened by coming to a
largely negative result.&lt;/p&gt;

&lt;p&gt;If there&amp;#39;s interest, I&amp;#39;ll try and do another analysis piece
speculating on the price of DOGE and our required hashrate through
halvenings, but that&amp;#39;s for another day.&lt;/p&gt;

&lt;p&gt;^d&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Dogecoin part I</title>
    <link href="http://arrdem.com/2014/04/24/dogecoin-i/"/>
    <updated>2014-04-24T00:00:00+00:00</updated>
    <id>http://arrdem.com/2014/04/24/dogecoin-i</id>
    <content type="html">&lt;p&gt;This post originally appeared on reddit.com/r/dogecoin, and was added
to my official tech blog for posterity. or something.&lt;/p&gt;

&lt;p&gt;&lt;hr /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;WARNING: WALL OF TEXT, HIGH SCIENCE CONTENT&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Friends, shibes, it is my pleasure to speak with you for what I hope
is the first and not the last time. I&amp;#39;m /u/arrdem [1] , I&amp;#39;m a Doge
daytrader, economist and miner on the side, and a programmer during
the day. Today I&amp;#39;d like to have a chat about some of the rumors with
regards to ASICs and the X11 hash that have been floating around
/r/dogecoin[2] for the last few weeks and I hope bring some light to
the discussions.&lt;/p&gt;

&lt;h2&gt;On Scrypt&lt;/h2&gt;

&lt;p&gt;What is special about our hash function? Why does Bitcoin use SHA256
and why does Doge use Scrypt? The hash function used by each
cryptocurrency must have no known inverse function or algorithmic
weakness which allowing miners to cheat and compute nonces easily, and
it needs to be easy to verify or recompute given an input. The first
requirement is obvious in that if the hash function is weak, then
someone can achieve a 51% attack potentially with less than 51% of the
network&amp;#39;s hashing power. The second is less obvious and is in fact
entirely a performance issue.&lt;/p&gt;

&lt;p&gt;SHA256 is a known and trusted algorithm which has yet to exhibit any
known weaknesses, and it is very very fast to recompute. This is why
Bitcoin is SHA based.&lt;/p&gt;

&lt;p&gt;Litecoin, the intellectual father of Dogecoin, chose the Scrypt hash
function because it was a memory bound algorithm. That is, the slowest
part of computing the Scrypt hash of some value is waiting for values
to be fetched from memory: an operation which it is amazingly
expensive to make fast. The goal of choosing an artificially expensive
hash function was to escape the Application Specific Integrated
Circuits (ASICs or hardware miners) which had come to dominate Bitcoin
mining. Because the SHA256 algorithm does not have large memory
requirements, it was easy for Bitcoin speculators to develop cost
effective hardware for the single purpose of searching for SHA256
nonce values.&lt;/p&gt;

&lt;h2&gt;On ASICs&lt;/h2&gt;

&lt;p&gt;Before we get to whether ASICs are good or bad for a coin, we must
first assess why they made sense for Bitcoin so that we can reason
about their impact on Doge.&lt;/p&gt;

&lt;p&gt;Because the computational power to find a nonce for any good
cryptocurrency is expected to be large, that means there is a literal
cost attached to processing each transaction on the network. While
transactions may be nominally free or at least low fee, miners are
really speculators expecting that someday the value of the coins they
earn computing nonce values for blocks will exceed the operating costs
and purchase costs of the hardware they mine with. This expectation
that one day mining costs will be repaid is in fact the key reason
that Bitcoin featured block rewards. The block reward was seen as a
bootstrapping mechanic with which to buy hardware investment in the
Bitcoin network through currency inflation.&lt;/p&gt;

&lt;p&gt;Now, ASICs and other mining hardware only pay for themselves if one
expects to get enough return from block rewards and future coin price
increases to cover the purchase and operating costs of the
hardware. However, this is where the block schedule comes in. If we
expect that thanks to the
&lt;a href=&quot;http://en.wikipedia.org/wiki/Law_of_large_numbers&quot;&gt;law of large numbers&lt;/a&gt;
that one&amp;#39;s return is on average the block reward times ones fraction
of the network hashrate, it becomes clear that as the block reward
falls it becomes very difficult for any purchased mining hardware to
pay itself off let alone turn a profit.&lt;/p&gt;

&lt;h2&gt;On the block schedule&lt;/h2&gt;

&lt;p&gt;Looking at the
&lt;a href=&quot;https://en.bitcoin.it/wiki/Controlled_supply#Projected_Bitcoins_Long_Term&quot;&gt;Bitcoin block schedule&lt;/a&gt;,
ASICs kinda make sense. The Bitcoin block schedule extends until 2140,
at which time the &amp;quot;omega block&amp;quot; will be mined and the per block reward
of Bitcoin mining will become zero. However until that time the per
block reward will decrease 50% every four years. Today in 2014, the
per block reward of Bitcoin is 25BTC and it won&amp;#39;t change
until 2017. That means that Bitcoin targeted ASICs can potentially run
for three whole years or more and still have a reasonable chance of
breaking even with no assuptions made about changes in the value of
1BTC.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Dogecoin#Block_schedule&quot;&gt;Doge&amp;#39;s block schedule&lt;/a&gt;
looks completely different. Where Bitcoin has a long tail on its per
block reward extending out to 2140, Dogecoin will reach it&amp;#39;s minimum
block reward at block 600,000 in January of 2015, less than 14 months
after Dogecoin came into being. With the 3rd halvening about 11 days
out and the 4th on the horizon, by the time big boy ASICs for Scrypt
start shipping in Q3/Q4, being September and later, the per block
reward of Doge will have fallen to 31.25KDOGE and below. Third
generation ASICs slated for December and January will likely never see
more than 15.625KDOGE/block.&lt;/p&gt;

&lt;h2&gt;On the price of Doge&lt;/h2&gt;

&lt;p&gt;So what does this mean for the price of Doge? If the price of Doge
doesn&amp;#39;t increase at all, it&amp;#39;s clear that the expensive new ASICs will
never break even. This suggests that late comers with high powered
mining hardware will be looking to recoup their investments and asking
higher and higher prices for their Doge which should drive up the
price overall.&lt;/p&gt;

&lt;p&gt;To put some numbers on this, at current prices and hashrate,
accounting for halvenings, neither Gridseed ASIC even breaks even
within 200 days if purchased within the next
48hrs. &lt;a href=&quot;https://www.refheap.com/78071&quot;&gt;fn:1&lt;/a&gt;. Wait 30 days (after the
comming halvening) and you don&amp;#39;t come anywhere near break even. If I
change my model to include some hashrate growth factor, the outlook is
even worse. &lt;a href=&quot;https://www.refheap.com/78310&quot;&gt;fn:2&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This isn&amp;#39;t bad news. This is awesome news for the price of DOGE. Lets
say that Gridseed ships oh 500 units of their big boy ASIC, which may
be conservative. &lt;a href=&quot;https://www.refheap.com/78313&quot;&gt;fn:3&lt;/a&gt; That&amp;#39;s right,
if hardware equivalent to 1K large Gridseeds came on in the next 30
days and ran at least for 200, doge would have to go all the way up to
702DOGE/USD just for them to break even!&lt;/p&gt;

&lt;h2&gt;To the moon&lt;/h2&gt;

&lt;p&gt;So where does this leave us. I think that the numbers I&amp;#39;ve presented
here show that ASICs for Dogecoin are patently absurd, unless you
expect to see a gargantuan spike in the price of DOGE which would make
us all rich men anyway. While I&amp;#39;m willing to speculate on block reward
(which is easy to model) and on hashrate which I assume is more or
less linear, I have no mechanism with which I can confidently predict
the price of DOGE out more than a week. Naive linear projections from
our initial open of 80 satoshi to today&amp;#39;s 126 satoshi over the course
of four months suggests that in 200 days we could well see the ~300
satoshi prices which would make Gridseed and other ASIC miners
profitable. However once you account for the high volatility of Doge,
of Bitcoin and general market manipulation who knows if it&amp;#39;d ever go
that high stably.&lt;/p&gt;

&lt;p&gt;So. To sum up. On the basis of these sketchy ROI numbers, I think that
buying ASICs is probably ill advised. That said, I expect that people
will buy ASICs and that in doing so they will drive up the price of
DOGE at the same time as the supply of DOGE starts to dry up due to
block reward decreases.&lt;/p&gt;

&lt;p&gt;I will be interested to see what happens to DOGE mining in January, as
we will be the first coin to reach their steady mining state. I hope
that the 10,000 DOGE reward per block will be sufficient to support
the ASIC and GPU mining required to keep our hashrate out of 51%
threat, but only time will tell. There is a real threat that the ROI
of mining will be too low to justify the purchase of new ASIC let
alone GPU hardware, which would lead to a falling hashrate and a
credible threat of 51% vulnerability. However we could also see prices
to go to the moon in which case that is no worry as high efficiency
ASIC farms would take over mining securing the coin&amp;#39;s stability more
or less. I will note that no coin has yet solved the 51% threat issues
posed by centralized mining, and I&amp;#39;m personally convinced that it&amp;#39;s an
intractable problem because as rewards per block decrease as for
bitcoin, the costs of mining operations must likewise fall leading to
greater centralization of compute power. By fixing our block reward we
may. may. be able to dodge (ha ha) this issue however the essential
drive to cut mining prices for ROI maximization will remain and will
continue to drive mining centralization.&lt;/p&gt;

&lt;p&gt;With all this in mind, it&amp;#39;s silly to talk about the adoption of X11 or
another hashing algorithm, because if and when ASIC miners for DOGE
become big business it&amp;#39;ll already be too late and we will have already
mined the vast majority of DOGE thus securing the distribution of DOGE
away from the ASIC miners we seem to fear so much as a
community. Making the switch to X11 simply delays the ASIC hardware
which we want anyway due to the price increases it&amp;#39;s likely to drive,
forget about making us artificially dependent on GPU mining to secure
our hashrate and creating an uncalled for blockchain fork.&lt;/p&gt;

&lt;h2&gt;TL;DR&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Stop worrying and love the ASICs, they won&amp;#39;t make a ton of money and will secure our hashrate and by proxy our Doges!&lt;/li&gt;
&lt;li&gt;STFU about X11. It&amp;#39;s even more ASIC friendly than Scrypt, and we gain nothing from another blockchain fork.&lt;/li&gt;
&lt;li&gt;Price projection: moon!&lt;/li&gt;
&lt;li&gt;Open issue: How do we limit mining centralization without increasing inflation? Are we already at a balance point?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;MSC&lt;/h2&gt;

&lt;p&gt;The software I&amp;#39;ve built and used to make these models is entirely open
source and written in Clojure, see the footnotes for source and
libraries.&lt;/p&gt;

&lt;h2&gt;Other programs involved&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.refheap.com/78314&quot;&gt;https://www.refheap.com/78314&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/arrdem/meajure&quot;&gt;https://github.com/arrdem/meajure&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Edit History&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Wording typo fixed&lt;/li&gt;
&lt;li&gt;Fix fn:2 to reflect increased network hashrate&lt;/li&gt;
&lt;li&gt;Don&amp;#39;t bother asking me what I think the price of DOGE will be. Not the foggiest.&lt;/li&gt;
&lt;li&gt;Fix final block reward, 10k not 100k&lt;/li&gt;
&lt;li&gt;Fix omega block date for BTC, 2140 not 2024&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;^d&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Batbridge, an introduction to computer architecture</title>
    <link href="http://arrdem.com/2014/01/10/Batbridge/"/>
    <updated>2014-01-10T00:00:00+00:00</updated>
    <id>http://arrdem.com/2014/01/10/Batbridge</id>
    <content type="html">&lt;p&gt;The history of computing machines is long and complex. This time last
year, I had finished the single class I&amp;#39;ve enjoyed most in my academic
career to date: CS 352H, &amp;quot;Computer Architecture&amp;quot;. In this class, we
explored at great length the history of computing machines and the
various designs which have been developed and sequentially abandoned
in the pursuit of speed over the past decades.&lt;/p&gt;

&lt;p&gt;In this post series, I seek to re-learn, implement and share some of
the really interesting tricks which modern processors play to achieve
the blistering performance which we have the luxury of taking for
granted and to shed some light on the engineering history and train of
thought which brings us to the modern day.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;/2014/01/01/Hardware-101/&quot;&gt;Hardware 101&lt;/a&gt; - Latches, Memories and why Offchip Sucks&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2014/01/02/a-stack-machine/&quot;&gt;stack&lt;/a&gt; and tape machines&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2014/01/03/a-single-cycle-machine/&quot;&gt;register&lt;/a&gt; machines&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2014/01/04/a-cache-higherarchy/&quot;&gt;cache&lt;/a&gt; accelerated machines&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2014/01/05/a-pipeline-machine/&quot;&gt;pipelined&lt;/a&gt; machines&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2014/01/06/a-branch-predicted-machine/&quot;&gt;branch predicted&lt;/a&gt; machines&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2014/01/07/a-superscalar-machine/&quot;&gt;superscalar&lt;/a&gt; machines&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2014/01/08/an-out-of-order-machine/&quot;&gt;out of order&lt;/a&gt; machines&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Throughout these posts, I will take a literate programming approach in
that I will attempt to interleave code with the rationale
therefore. Each simulator which I develop will be standalone in terms
of source code and will be published
&lt;a href=&quot;https://github.com/arrdem/batbridge&quot;&gt;here on github&lt;/a&gt; for your benefit
and ease of access.&lt;/p&gt;

&lt;p&gt;^d&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Building an out of order machine</title>
    <link href="http://arrdem.com/2014/01/08/an-out-of-order-machine/"/>
    <updated>2014-01-08T00:00:00+00:00</updated>
    <id>http://arrdem.com/2014/01/08/an-out-of-order-machine</id>
    <content type="html">&lt;p&gt;Okay. Previously we tried to get more parallelism and work per cycle
out of our machines by running two streams of instructions more or
less in parallel by having two parallel synchronized pipelines. But we
hit a snag with this: data dependencies in between pipelines forced us
to stall, thus loosing us whatever benefit we had hoped to reap from
instruction level parallelism.&lt;/p&gt;

&lt;p&gt;We can make some performance gains by informing compiler writers of
the internal workings of our pipeline(s) and giving them hints about
how best to arrange instructions so as to minimize stalls incurred by
data dependencies, but there are limits to how insane compilers and
compiler writers are willing to go in optimizing for a specific
architecture. Besides, what if we could detect instruction level
parallelism in hardware and take advantage of it in hardware rather
than relying on smarter compilers to play those tricks on our behalf.&lt;/p&gt;

&lt;p&gt;Well, how did we do data dependency detection? We retained a small
data structure representing the registers to which changes were being
computed and we stalled the pipeline until the requisite changes had
been fully realized. So what happens when we have an instruction N
which suffers from a data dependency, and an N+1 that is not so bound?
N+1 gets hosed and has to wait in line until N clears the pipeline. We
can do better than this! By creating some sort of structure
representing instructions which are waiting to be executed, we can
fill the execution pipeline every cycle by simply selecting the next
instruction which is not data bound.&lt;/p&gt;

&lt;p&gt;But what about our programming model? As long as we&amp;#39;ve been designing
hardware the assumption is that if the Nth instruction makes a change
all instructions after that Nth instruction see the steady state left
by the Nth instruction. Therefor if we allow the N+2nd instruction to
make visible changes before the Nth instruction, we have violated our
programming model and programs may break! To resolve this we need to
be able to reorder results. We will add a 5th stage to our pipeline,
in between execution and writeback, which buffers writeback commands
and only allows them to be written back in the order of nominal single
cycle instruction execution.&lt;/p&gt;

&lt;p&gt;What happens then if we have a data dependency between an instruction
which has already been executed out of order but has not escaped
reorder and another instruction in the issue buffer which would be
good to go if the value stalled in reorder were visible? Can we
somehow resolve this unneeded stall? In order to resolve this, we
need some way to make buffered reorder values visible to the out of
order instruction stream in addition to the values in registers. Know
what the reorder stage is starting to sound like? Another register
array! What if we extended our register set to include a large number
of programmer-invisible registers, and then achieved out of order
behavior by translating the operands of instructions in the issue
buffer to aliases in this range of hidden registers. So an addition
operation which nominally reads registers 0 and 1, storing its result
to register 2 may in fact read from two arbitrary hidden registers
containing the out of order &lt;em&gt;future&lt;/em&gt; values of registers 0 and 1, and
then write the &lt;em&gt;future&lt;/em&gt; value of register 2 to another alias register
storing a mapping from this alias for the future value of 2 to the
register 2 as the buffered writeback command in the reorder stage.&lt;/p&gt;

&lt;p&gt;Now get this: as long as this mapping from architectural register 2 to
the renamed future value of register 2 is preserved, the issue stage
is free to take the &lt;em&gt;next&lt;/em&gt; instruction which reads the value of
architectural register 2, rewrite it to read from the alias for the
future value of register 2 and execute it, writing back to a new alias
register. So long as the out of order processor does not run out of
space in its alias table and continues to write back renamings from
the reorder stage this processor can follow individual data dependency
streams arbitrarily far into the future!&lt;/p&gt;

&lt;p&gt;Now this comes with a caveat: really awesome branch prediction now
becomes even more crucial, and we need some way to uniquely and
monotonically identify instructions in issue and reorder with respect
to some previous state of the PC. This is all because if a jump is
misspredicted, then correcting the missprediction involves dumping all
processor the internal state representing staged future values which
are now incorrect with respect to the confirmed steady state of the
processor.&lt;/p&gt;

&lt;p&gt;And that&amp;#39;s all I&amp;#39;ve got. There are more funky architectures out there,
but every major desktop and server processor in the last 20 years or
so can be constructed from these principles. There are some things
I&amp;#39;ve left out: vector processors and data flow processors, but vector
processors tend to be special purpose computers (graphics cards) and
data flow processors don&amp;#39;t exist yet so I&amp;#39;m content to call it a wrap
here. I hope you enjoyed the ride and learned something!&lt;/p&gt;

&lt;p&gt;^d&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Building a superscalar machine</title>
    <link href="http://arrdem.com/2014/01/07/a-superscalar-machine/"/>
    <updated>2014-01-07T00:00:00+00:00</updated>
    <id>http://arrdem.com/2014/01/07/a-superscalar-machine</id>
    <content type="html">&lt;p&gt;Thanks to branch prediction, we now have nice, full pipelines. But
while we can make pipelines deeper still as the Intel Pentium IV
architecture is evidence building a 36 stage pipeline just hurts no
matter how clever you think you are about predicting branches. So we
need some new technique for doing yet more work every cycle.&lt;/p&gt;

&lt;p&gt;How about running two instructions at a time? If we could run two
pipelines side by side on one chip, one operating at PC and the other
at PC+1, and both incriminating PC by 2 every cycle as long as the Nth
and N+1th instructions don&amp;#39;t suffer from a data dependency the we can
totally run two instructions entirely in parallel! Congratulations,
that&amp;#39;s a superscalar architecture. In fact there is exactly nothing
more to superscalar than running two or more pipelines side by side.&lt;/p&gt;

&lt;p&gt;However the data hazard logic becomes more complicated from this added
parallelism. Now we have to check that the instructions in all Kth
parallel pipelines do not depend data-wise on the results partially
computed anywhere else in the parallel pipeline system. Yeah, this
gets messy quickly.&lt;/p&gt;

&lt;p&gt;Also control flow gets messy fast like this. If the Nth instruction is
a branch, and the N+1st instruction performs some operation, how can
we synchronize the pipelines to make sure that if the N+1st
instruction should not be executed according to the results of the Nth
instruction. While it is possible to introduce the hardware level
synchronization between pipelines required for a branch in one to
squash operation(s) in another, doing so is troublesome. For this
reason the MIPS specification explicitly states that the instruction
&lt;em&gt;after&lt;/em&gt; a branch must be assumed to be executed. For this reason it is
common to see branch instructions in MIPS followed by a no-op operation :D&lt;/p&gt;

&lt;p&gt;Well now we&amp;#39;ve done all that we can with pipelines and
prediction.. we&amp;#39;ve tried multiple pipelines in parallel with limited
success and it seems like we&amp;#39;re going to have to take a more concerted
stab at slaying the data dependency beast in order to fill our
pipelines to the brim all the time. Until
&lt;a href=&quot;/2014/01/08/an-out-of-order-machine/&quot;&gt;next time&lt;/a&gt;
when we slay the best with out of order execution!&lt;/p&gt;

&lt;p&gt;^d&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Building a branch predicted machine</title>
    <link href="http://arrdem.com/2014/01/06/a-branch-predicted-machine/"/>
    <updated>2014-01-06T00:00:00+00:00</updated>
    <id>http://arrdem.com/2014/01/06/a-branch-predicted-machine</id>
    <content type="html">&lt;p&gt;Last time I signed off with a question: Given a fetch stage which runs
many cycles ahead of the processor&amp;#39;s fully evaluated steady state, how
can we ensure that the fetch stage is filling the pipeline with useful
instructions and not just wasting time filling the processor with crap
that will have to be thrown away?&lt;/p&gt;

&lt;p&gt;In order for such a thing to be possible, we must somehow be able to
predict the state which the processor will reach after performing
computations, without performing the requisite computation. We must,
quite literally, predict the future. The accuracy with which we can do
this will dramatically impact the performance of our processors by
ensuring that we waste as few cycles as possible.&lt;/p&gt;

&lt;p&gt;What makes us waste cycles in a pipelined processor design? We have
value hazards which we already partially dealt with, and we have
control hazards. Control hazard is the term for instructions fetched
&lt;em&gt;after&lt;/em&gt; a control flow (also known as a conditional or branch)
instruction is fetched which will impact whether the instructions
should be executed or not. Say we define the &amp;quot;if*&amp;quot; set of instructions
to execute the N+1 instruction if and only if the predicate is true,
and to otherwise execute the N+2 instruction, skipping the N+1.&lt;/p&gt;

&lt;p&gt;Given these branching semantics, the four level pipeline which we
described above will have fetched the N+2 instruction and decoded the
N+1 instruction when the control instruction at N is
computed. However, it is not until the next cycle that the outcome of
the Nth instruction is visible to the rest of the processor, by which
time the N+1 instruction will have completed execution and the N+2
instruction will have completed decode.&lt;/p&gt;

&lt;p&gt;In the case of this processor, it makes sense to allow the N+1 and N+2
instructions to be executed and decoded because adding extra control
logic to remove (or squash) whichever instruction the outcome of the
branch instruction indicates is not correct is trivial. However, in
the general case of a deeper pipeline wherein it is possible that an
entire loop iteration or loop exit sequence has been fetched and
partially decoded/executed it makes no sense to add the complicated
instruction removal logic to every stage. Instead the decode stage
will detect that the instruction falls into the set of branching
instructions and will disable the fetch stage until the conditional
instruction has cleared writeback. Clearly this would be painful for
our simple pipeline, for a 16 or 32 cycle pipeline this branching cost
is positively evil.&lt;/p&gt;

&lt;p&gt;So we need to guess two things: what the outcome of conditional
instructions will be, and what the next PC will be for every
instruction. The first of these turns out to be relatively easy. Most
processor time is spent inside of loops, and loops tend to terminate
with a &amp;quot;jump to the top&amp;quot; statement of some sort. Ergo, most of the
branches which a processor will encounter will be &amp;quot;jump to the top&amp;quot;
branch instructions which are far more often than not taken. In fact
this bias towards taking &amp;quot;backwards&amp;quot; jumps is so strong that simply
predicting taken on every branch can yield a prediction success rate
of 80%!&lt;/p&gt;

&lt;p&gt;Okay great. Now we can make at least a crude estimation of whether a
given branch will be taken or not. However, for the branch
instructions described this information isn&amp;#39;t very valuable. Because
the &amp;quot;true&amp;quot; case is to execute the N+1 instruction and the &amp;quot;false&amp;quot; case
is to skip N+1 and go for N+2, most code on this machine will have the
form&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-clojure&quot; data-lang=&quot;clojure&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:if..&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;* &lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;* &lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;* &lt;/span&gt;  &lt;span class=&quot;nb&quot;&gt;* &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:add&lt;/span&gt;  &lt;span class=&quot;nv&quot;&gt;pc&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pc&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;imm&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;* &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;; jump to true case&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:add&lt;/span&gt;  &lt;span class=&quot;nv&quot;&gt;pc&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pc&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;imm&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;* &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;; jump to false case&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;or alternatively&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-clojure&quot; data-lang=&quot;clojure&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:if..&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;* &lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;* &lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;* &lt;/span&gt;  &lt;span class=&quot;nb&quot;&gt;* &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:add&lt;/span&gt;  &lt;span class=&quot;nv&quot;&gt;pc&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pc&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;imm&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;* &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;; jump to the true case&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;....................&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;; inline false case&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that this means we more than likely have two back to back control
hazards! Oh joy. So we can somewhat correctly predict the outcome of a
branch, but that doesn&amp;#39;t help the fetch stage fill our pipeline with
useful instructions. So we need some more general structure for
predicting the PC of the next instruction. Normally this is simple,
the PC of the next instruction is simply the PC+4. However, as
demonstrated with the instructions following conditionals, a highly
meaningful set of control hazards are not presented by conditional
instructions. This means that we need some more general structure for
mapping the PC of one instruction to the PC of the next
instruction. We can accomplish this by simply having a table which
maps the entire valid PC address range [0, INT_MAX] to the PC which
followed that address last time an instruction at that address was
executed.&lt;/p&gt;

&lt;p&gt;This however is problematic. Clearly we cannot build such a table,
because it would be exactly the size of main DRAM memory. Thus the
access time to our predictive data store would be as poor as the access
time to our main memory! Entirely unacceptable! We want our branch
prediction structure to be so fast that it can run alongside fetch!
So now we start playing tricks to reduce the table size and improve
the predictor performance. Clearly, most of a processor&amp;#39;s memory is
used for storing data not for storing code so right off the bat we
know that our worst case size is overkill. Also we know that as
programs tend to spend most of their time in tight inner loops the
address range for which we must be able to issue next address
predictions is rather small. Also we know that most of the space in
that table of next instruction addresses would be wasted because the
overwhelming majority of addresses would simply indicate N+1. This
suggests that we really want to be making two predictions: 1) is the
next PC N+1, and if not 2) what is the next pc?&lt;/p&gt;

&lt;p&gt;To make this first prediction, I propose that we adapt the classic
bimodal branch predictor from
&lt;a href=&quot;http://www.hpl.hp.com/techreports/Compaq-DEC/WRL-TN-36.pdf&quot;&gt;McFarling&lt;/a&gt;. The
bimodal predictor is bloody simple and can achieve a prediction
accuracy of 94% or better. We will define a bimodal predictor table of
1024 entries, and on every fetch we will consult the table to see
whether it predicts taken or not taken for the current index, computed
by taking the bottom 9 bits of the current PC. If the prediction is
not taken, then the next PC is PC+1.&lt;/p&gt;

&lt;p&gt;By mapping PCs to two bit counters with values &lt;code&gt;[0,1]&lt;/code&gt; representing
branch not taken and &lt;code&gt;[2,3]&lt;/code&gt; representing branch taken. Using this
representation, given a single counter we can predict whether the next
PC of an instruction is PC+4 or whether it is some other value.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/arrdem/batbridge/blob/master/src/batbridge/predicted_pipeline.clj#L23&quot;&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-clojure&quot; data-lang=&quot;clojure&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;defn &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;two-bit-counter&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;op&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:inc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;inc &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;or &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:dec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dec &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;or &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Not to hard to build, but now we need to build a predictive structure
out of these cells. Remember that we are using a bit vector (integer)
addressed table of cells to map from what are essentially hashed
&lt;code&gt;(history, PC)&lt;/code&gt; pairs to a counter, so to train a specific counter one
way or another we have to compute the table index and then delegate to
&lt;code&gt;two-bit-counter&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/arrdem/batbridge/blob/master/src/batbridge/predicted_pipeline.clj#L38&quot;&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-clojure&quot; data-lang=&quot;clojure&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;defn &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;train-pred&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;addr&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;hst&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;let &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;bit-xor &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;bit-and &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;x1FF&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;addr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vec-&amp;gt;bitv&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;hst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]]&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;res&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:taken&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;update-in&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;two-bit-counter&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:inc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:not-taken&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;update-in&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;two-bit-counter&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:dec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Great. So we can represent a table of our counters, but we need one
more piece, a way to make a decision from the counter table. Remember
that we defined &lt;code&gt;[0,1]&lt;/code&gt; to be not taken and &lt;code&gt;[2,3]&lt;/code&gt; to be taken, so we
can implement this as follows:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/arrdem/batbridge/blob/master/src/batbridge/predicted_pipeline.clj#L46&quot;&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-clojure&quot; data-lang=&quot;clojure&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;defn &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;predict-pred&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;addr&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;hst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;&amp;gt;= &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;get &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;bit-xor &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;addr&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vec-&amp;gt;bitv&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;hst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Okay, now we need to fit this predictor structure into our existing
processor. In order to do this, I&amp;#39;m simply going to create the
&lt;code&gt;:predictor&lt;/code&gt; substructure in the processor state, which will feature
the keys &lt;code&gt;#{:hst, :pred :jump-map}&lt;/code&gt;. &lt;code&gt;:hst&lt;/code&gt; will be a sequence of
booleans representing the branch history, &lt;code&gt;:pred&lt;/code&gt; will be our two bit
counter table, and &lt;code&gt;:jump-map&lt;/code&gt; will map PCs to their last jump target.&lt;/p&gt;

&lt;p&gt;With this definition in hand we can write two training helpers
functions to let us update the two bit counter tables in a given
processor instance...&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/arrdem/batbridge/blob/master/src/batbridge/predicted_pipeline.clj#L96&quot;&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-clojure&quot; data-lang=&quot;clojure&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;defn &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;train-jump&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;let &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:keys&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;hst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:predictor&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;-&amp;gt; &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;update-in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:predictor&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;train-pred&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;hst&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:taken&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;update-in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:predictor&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:jump-map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;assoc &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;


&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;defn &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;train-step&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;let &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:keys&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;hst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:predictor&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;nv&quot;&gt;this&lt;/span&gt;          &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;common/get-register&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;31&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;-&amp;gt; &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;update-in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:predictor&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;train-pred&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;hst&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:not-taken&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;now we need to be able to update the processor branch history...&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/arrdem/batbridge/blob/master/src/batbridge/predicted_pipeline.clj#L88&quot;&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-clojure&quot; data-lang=&quot;clojure&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;defn &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;update-history&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;outcome&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;update-in&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:predictor&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:hst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
             &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;fn &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;into &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ring-buffer&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                     &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;conj &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;hist&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;outcome&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))))&lt;/span&gt;


&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;defn &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;update-taken&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;update-history&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;


&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;defn &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;update-not-taken&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;update-history&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Finally, we need to be able to predict the next PC value using these
various helpers and the predictive structure we&amp;#39;ve defined.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/arrdem/batbridge/blob/master/src/batbridge/predicted_pipeline.clj#L52&quot;&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-clojure&quot; data-lang=&quot;clojure&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;defn &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;next-pc&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&amp;quot;Examines a processor state, determining the next value for the&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;  PC. Note that this relies on support from writeback to record PC&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;  value transitions when jumps are executed.&amp;quot;&lt;/span&gt;

  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;let &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:keys&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;jump-map&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;hst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:predictor&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;nv&quot;&gt;pc&lt;/span&gt;                          &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;common/get-register&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;31&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;and &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;contains? &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;jump-map&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
             &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict-pred&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pc&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;hst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;get &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;jump-map&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;+ &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And that&amp;#39;s all the code we need to be able to interact with the
processor&amp;#39;s predictor structures. Now we need to implement a fetch
stage which will query the branch predictor structure to determine
what the address of the next instruction is if it isn&amp;#39;t PC+4.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/arrdem/batbridge/blob/master/src/batbridge/predicted_pipeline.clj#L109&quot;&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-clojure&quot; data-lang=&quot;clojure&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;defn &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;fetch&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&amp;quot;Checks the stall counter, invoking ss/fetch if zero otherwise&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;  returning the input state unmodified due to a pipeline stall.&amp;quot;&lt;/span&gt;

  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;p/stalled?&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;let &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;pc&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;common/get-register&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;31&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;nv&quot;&gt;icode&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;common/get-memory&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;nv&quot;&gt;npc&lt;/span&gt;   &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;next-pc&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;info&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;[fetch    ]&amp;quot;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pc&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;-&amp;gt;&amp;quot;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;icode&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot; npc:&amp;quot;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;npc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;-&amp;gt; &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;assoc-in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:registers&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;31&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;npc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;assoc &lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:fetch&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:icode&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;icode&lt;/span&gt;
                         &lt;span class=&quot;ss&quot;&gt;:npc&lt;/span&gt;   &lt;span class=&quot;nv&quot;&gt;npc&lt;/span&gt;
                         &lt;span class=&quot;ss&quot;&gt;:pc&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;+ &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;pc&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)})))))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Note that this implementation has to preserve the original pipelined
processor&amp;#39;s behavior of respecting the stalled state of the pipeline,
in addition to providing next PC lookup from our branch predictor
structure. Also note that I have introduced the &lt;code&gt;:npc&lt;/code&gt; key to the
fetch value. This key obviously represents the predicted address of
the next instruction as it is no longer strictly equal to PC+4. This
is important because when we get to writeback we will need to be able
to determine whether or not a branch really needs to generate a flush
and a jump.&lt;/p&gt;

&lt;p&gt;Now lets kick out the last interesting piece of this simulator, the
writeback stage. Writeback for branch predicted processors takes
responsibility for training, and needs to deal with a logical case
that our previous writeback stages have ignored. Previously, we&amp;#39;ve
assumed that jumps will never target the fetch PC of the next
instruction in the pipeline. However, if branch prediction is working
perfectly, the next instruction in the pipeline will &lt;em&gt;always&lt;/em&gt; be the
target of the preceding branch. If we correctly branch to the
predicted branch target, then we want to positively train our
predictive structure because it made the right guess. If we branch to
somewhere else, then we have to incur a full pipeline stall (ouch!)
and train the predictive structure not to make the same mistake
again. However in the general case of &amp;quot;normal&amp;quot; instructions which do
not cause branching behavior we don&amp;#39;t need to update the predictor at
all because the predictor assumes that any instruction for which it
does not have prior information the next PC is PC+4, which is correct
for all non branch instructions. As we have already dealt with
training branches, we don&amp;#39;t need to do anything more training wise.&lt;/p&gt;

&lt;p&gt;However, for every instruction processed we must update our branching
history, which means that when we process a branch instruction we have
to add a &lt;code&gt;1&lt;/code&gt; to the history sequence, and otherwise we add an &lt;code&gt;0&lt;/code&gt;
respectively using &lt;code&gt;update-taken&lt;/code&gt; and &lt;code&gt;update-not-taken&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;That&amp;#39;s it, so lets build a new writeback...&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/arrdem/batbridge/blob/master/src/batbridge/predicted_pipeline.clj#L127&quot;&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-clojure&quot; data-lang=&quot;clojure&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;defn &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;writeback&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&amp;quot;Pulls a writeback directive out of the processor state, and&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;  performs the indicated update on the processor state. Update command&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;  have been restructured and are now maps&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;  {:dst (U :registers :halt :memory) :addr Int :val Int}.&amp;quot;&lt;/span&gt;

  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;let &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;directive&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;get &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:execute&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:registers&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:keys&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;dst&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;addr&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;val &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;pc&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;npc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;directive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;info&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;[writeback]&amp;quot;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;directive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cond &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;;; special case to stop the show&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;= &lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:halt&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;dst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;assoc &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:halted&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

          &lt;span class=&quot;c1&quot;&gt;;; special case for hex code printing&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;and &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;= &lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:registers&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;dst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;= &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;29&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;addr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;do &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;when-not &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zero? &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;print &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;format&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;0x%X&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;char &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;
                &lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

          &lt;span class=&quot;c1&quot;&gt;;; special case for printing&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;and &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;= &lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:registers&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;dst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;= &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;addr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;do &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;when-not &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zero? &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;print &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;char &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
                &lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

          &lt;span class=&quot;c1&quot;&gt;;; special case for branching as we must flush the pipeline&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;and &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;= &lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:registers&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;dst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;= &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;31&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;addr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;not &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;= val &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;npc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;; don&amp;#39;t flush if we aren&amp;#39;t changing&lt;/span&gt;
                                  &lt;span class=&quot;c1&quot;&gt;;; the next PC value. This means to&lt;/span&gt;
                                  &lt;span class=&quot;c1&quot;&gt;;; jumping to PC+4 does exactly&lt;/span&gt;
                                  &lt;span class=&quot;c1&quot;&gt;;; nothing as it should.&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;do &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;warn&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;[writeback] flushing pipeline!&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;-&amp;gt; &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;update-taken&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;train-jump&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;- &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;pc&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dissoc &lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:fetch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dissoc &lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dissoc &lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;assoc-in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:registers&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;addr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

          &lt;span class=&quot;c1&quot;&gt;;; special case for correctly predicted branching&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;and &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;= &lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;:registers&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;dst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;= &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;31&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;addr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;= val &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;npc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;;; In this case we need to reinforce the&lt;/span&gt;
                            &lt;span class=&quot;c1&quot;&gt;;; branch prediction.&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;-&amp;gt; &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;update-taken&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;train-jump&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;- &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;pc&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

          &lt;span class=&quot;nv&quot;&gt;true&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;-&amp;gt; &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;processor&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;update-not-taken&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;assoc-in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;dst&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;addr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And that&amp;#39;s it! We only need a &lt;code&gt;step&lt;/code&gt; function and a &lt;code&gt;-main&lt;/code&gt; function,
neither of which is different at all from the previous pipeline
implementation(s) save for making reference to this new &lt;code&gt;fetch&lt;/code&gt; and
&lt;code&gt;writeback&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now we can start to play more tricks with using more complicated
branch predictors. In fact building better branch predictors is still
an active research area. We can also play tricks with tracking more
processor state. Real world branch predictors play all kinds of crazy
tricks. One example of this is loop trip count prediction.&lt;/p&gt;

&lt;p&gt;Branch prediction works well for frequently repeated loops, however
the branch predictor presented here will always be wrong for the last
iteration of a loop. We could imagine a branch predictor which
determines the two values used in the loop back edge test and predicts
how many more times the program will execute the loop exactly thus
escaping the exit misprediction.&lt;/p&gt;

&lt;p&gt;Now that we can fill the pipeline with mostly useful instructions, we
need to find another avenue for doing more work per cycle. Next time
we&amp;#39;ll look at a technique for exploiting instruction level
parallelism: superscalar architectures!&lt;/p&gt;

&lt;p&gt;^d&lt;/p&gt;
</content>
  </entry>
  
</feed>
